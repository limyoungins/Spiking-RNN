{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43712388-c442-4021-bc0f-e898d4323078",
   "metadata": {},
   "source": [
    "# Analogous Recurrent ANN Trainer\n",
    "\n",
    "This notebook can be used to generate and train a recurrent ANN so that the weights can be copied over to a SNN with the same architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bf3b9-a1df-4af0-8122-1af8a12c46d8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf949556-79d1-4121-b739-50c0ca083181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9dc2525-0d6b-4117-b86e-7a982c6a8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(42397)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af0fa6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502bd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data and prepare for vector conversion\n",
    "\n",
    "# load data\n",
    "f = open(\"..\\\\Data\\\\Training Data\\\\train_5500.txt\")\n",
    "data = f.read()\n",
    "\n",
    "# split data into sentences\n",
    "sents = data.split('\\n')\n",
    "\n",
    "# split each sentence into words\n",
    "for i in range(len(sents)):\n",
    "    sents[i] = sents[i].split(' ')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e67a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare word2vector vocabulary (ty chatGPT :) )\n",
    "\n",
    "def read_word_vectors(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, vector_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * vector_size\n",
    "        word_vectors = {}\n",
    "\n",
    "        for _ in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == b' ':\n",
    "                    break\n",
    "                if ch != b'\\n':\n",
    "                    word.append(ch)\n",
    "            word = b''.join(word).decode('utf-8')\n",
    "            vector = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "\n",
    "    return word_vectors\n",
    "\n",
    "def get_word_vector(word, word_vectors):\n",
    "    return word_vectors.get(word)\n",
    "\n",
    "# Load the word vectors\n",
    "word_vectors = read_word_vectors('..\\\\Data\\\\true_vectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "858d6c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "Word: example\n",
      "Vector: [ 0.30070093 -0.14708109 -0.05512658 -0.28891692  0.16964374 -0.26381055\n",
      " -0.1238706  -0.26599023  0.04002719 -0.0139405  -0.09649079 -0.20608869\n",
      " -0.34638372 -0.05804674  0.373583   -0.03040116  0.1421226  -0.1942786\n",
      "  0.09814846 -0.19097279 -0.11167672  0.28619     0.06731004  0.33642802\n",
      "  0.16544527  0.12827992 -0.06165301 -0.15542434  0.32104218  0.01503407\n",
      "  0.39649448 -0.06764489  0.31964797  0.18220599 -0.2872599  -0.03778282\n",
      "  0.00464729  0.37776193  0.05840794 -0.00786143 -0.40444744  0.2015574\n",
      "  0.5840568   0.18758695  0.04742671  0.32666302 -0.1868712   0.30280098\n",
      "  0.237733   -0.6091492  -0.08265247  0.44745898 -0.14270085 -0.6882093\n",
      "  0.05940733 -0.14634833 -0.03241995  0.115207   -0.07995818 -0.19041005\n",
      "  0.2315593   0.15892205  0.13351633 -0.43538508]\n",
      "\n",
      "(64,)\n",
      "Word: word\n",
      "Vector: [ 0.21292834 -0.45759308 -0.18516563  0.12465494  0.02143879 -0.23055013\n",
      " -0.14937697 -0.43313178  0.23816882 -0.33727258 -0.25391486 -0.3715287\n",
      " -0.39470986  0.10469216  0.20358714  0.4016464  -0.06567805 -0.598168\n",
      " -0.0976235   0.31763846 -0.06485635  0.4069338   0.03542383  0.4290343\n",
      "  0.27986327  0.1338212   0.06545134  0.12722027  0.33235726 -0.3979507\n",
      "  0.06461156 -0.16785187  0.05850044  0.24267069 -0.07968269  0.10410772\n",
      " -0.11200889  0.27143833  0.2970559  -0.17555033 -0.30849037  0.24875005\n",
      "  0.31966418  0.6616499   0.02107978  0.4193211   0.02034558  0.4646038\n",
      "  0.37174353 -0.6534913  -0.07925424  0.7824209  -0.46214998 -0.76425076\n",
      "  0.2140789   0.2952882   0.0640111  -0.16001254  0.02353365  0.25328454\n",
      "  0.12017623  0.54661894  0.00523374 -0.38747782]\n",
      "\n",
      "(64,)\n",
      "Word: vector\n",
      "Vector: [ 0.7096468   0.516577   -0.5810915   0.18414372 -0.35334316 -0.21822394\n",
      " -0.23718251  0.06331351  0.04706604 -0.3053193   0.1293704  -0.83765876\n",
      " -0.07102425  0.632746    0.18350941 -0.6930966   0.04092861 -0.40495354\n",
      " -0.04916763 -0.5368738  -0.47088313  0.38114905  0.03184688 -0.46040407\n",
      "  0.42870155  0.7947799  -0.28688654 -0.3027485   0.03157321  0.55705076\n",
      "  0.3496584  -0.18566594 -0.20520718 -0.00520362 -0.946006   -0.3762953\n",
      " -0.21821569  0.28724688  0.5925276   0.08638821 -1.0233172   0.19916345\n",
      "  0.6459798  -0.0578034  -0.48934382  0.39450562 -0.3269895   0.36676836\n",
      "  0.60090095 -0.07156151  0.37199515  0.57098734  0.61894405 -0.8130763\n",
      "  0.11099844  0.26977542 -0.65597016  0.49208623 -0.0725408  -0.61878777\n",
      "  0.71139735  0.2647935   0.31502736 -0.4834256 ]\n",
      "\n",
      "(64,)\n",
      "Word: king\n",
      "Vector: [-0.2007563  -0.2662116  -0.01125938 -0.22246534  0.01673112 -0.16131678\n",
      "  0.5644422   0.2372719  -0.02554452 -0.4340164  -0.06115132  0.4474212\n",
      " -0.0169391   0.216706    0.12563393  0.4937512  -0.1108233  -0.22428668\n",
      "  0.28097486  0.04929437 -0.20667788 -0.01783528  0.42226025  0.6956192\n",
      "  0.36734813 -0.18279715  0.38456267  0.0937601   0.3472344   0.16173163\n",
      " -0.02196733 -0.30837262  0.35498407 -0.1253941  -0.34782177  0.26338297\n",
      "  0.3982433   0.2542479  -0.24443848 -0.25045708  0.12953766  0.35497877\n",
      "  0.44127497  0.3166054   0.21291043 -0.00193315  0.33634016  0.11970048\n",
      "  0.6741072  -0.2731611   0.01838719  0.62378085 -0.25395697 -0.42012843\n",
      " -0.15718731 -0.19368996 -0.4366203   0.61090666 -0.18145038 -0.22347619\n",
      "  0.3941599   0.6153464   0.01727371 -0.63497424]\n",
      "\n",
      "(64,)\n",
      "Word: queen\n",
      "Vector: [-0.02137611 -0.08973972  0.26116955 -0.19803847  0.10695402 -0.22895674\n",
      "  0.67390263  0.5426093  -0.09788782 -0.5653315  -0.13290218  0.65065336\n",
      " -0.27446434  0.06570383  0.5187515   0.7467789   0.22989054 -0.02971113\n",
      "  0.05134779  0.09393734 -0.31722817 -0.23058008 -0.08994956  0.5063627\n",
      "  0.26212266  0.03368724  0.44233537 -0.10973279  0.41281113  0.44937223\n",
      "  0.2203155  -0.11655781  0.02179532 -0.08874524  0.01851686 -0.2244643\n",
      "  0.41611004  0.0983404  -0.44452962 -0.46926522  0.01739598  0.3852077\n",
      "  0.3474639   0.1727038   0.3132215   0.10375009  0.13976559  0.13915694\n",
      "  0.4435009  -0.16649246 -0.08227239  0.35906744  0.00721158 -0.47048745\n",
      " -0.01934881 -0.07406729 -0.33092067  0.5696355   0.2876498  -0.3921086\n",
      "  0.4155587   0.6433693  -0.04171108 -0.98443013]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test vocabulary\n",
    "\n",
    "# Define a list of words to convert to vectors\n",
    "words = ['example', 'word', 'vector', 'king', 'queen']\n",
    "\n",
    "# Convert words to vectors\n",
    "for word in words:\n",
    "    vector = get_word_vector(word, word_vectors)\n",
    "    print(vector.shape)\n",
    "    if vector is not None:\n",
    "        print(f\"Word: {word}\\nVector: {vector}\\n\")\n",
    "    else:\n",
    "        print(f\"Word: {word} not found in vocabulary.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "402e0cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liamr\\AppData\\Local\\Temp\\ipykernel_20444\\131229499.py:12: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  vecs.append(torch.from_numpy(vec))\n"
     ]
    }
   ],
   "source": [
    "# perform word to vector conversion\n",
    "\n",
    "# perform conversion\n",
    "vec_data = []\n",
    "for sent in sents[:-1]:\n",
    "    vecs = []\n",
    "    vecs.append(sent[0])\n",
    "    for word in sent[1:]:\n",
    "        try:\n",
    "            word = word.lower()\n",
    "            vec = get_word_vector(word, word_vectors)\n",
    "            vecs.append(torch.from_numpy(vec))\n",
    "        except:\n",
    "            pass\n",
    "    vecs.append(torch.zeros(64))\n",
    "    vec_data.append(vecs)\n",
    "\n",
    "# pad all sentences to length of longest sentence\n",
    "max_len = max([len(sent) for sent in vec_data])\n",
    "vec_data_pad = []\n",
    "for sent in vec_data:\n",
    "    pad_len = max_len - len(sent)\n",
    "    for i in range(pad_len):\n",
    "        sent.append(torch.zeros(64))\n",
    "    vec_data_pad.append(sent)\n",
    "vec_data = vec_data_pad\n",
    "\n",
    "# split into training and test data\n",
    "train_data = vec_data[:5000]\n",
    "test_data = vec_data[5000:-1]\n",
    "\n",
    "# NOTE: first word of each sentence is correct categ. -- last sentence is empty (excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b78ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataSet which can be used with PyTorch DataLoader\n",
    "\n",
    "ans_key = { 'DESC' :  0,\n",
    "            'ENTY' :  1,\n",
    "            'ABBR' :  2,\n",
    "            'HUM'  :  3,\n",
    "            'LOC'  :  4,\n",
    "            'NUM'  :  5 }\n",
    "\n",
    "class QuestionDataset(Dataset):\n",
    "    \"\"\" Question Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        data = list of (list of words -- first word is label)\n",
    "        \"\"\"\n",
    "        self.labels = []\n",
    "        self.sents = []\n",
    "        for sent in data:\n",
    "            lab_val = ans_key[sent[0].split(\":\")[0]]\n",
    "            lab_arr = torch.tensor(lab_val)\n",
    "            self.labels.append(lab_arr)\n",
    "            self.sents.append(sent[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sent = self.sents[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sent, label\n",
    "\n",
    "train_DSet = QuestionDataset(train_data)\n",
    "test_DSet = QuestionDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482a8daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.0411, -0.0476,  0.3912,  0.1446,  0.2843,  0.0124, -0.0079, -0.0237,\n",
      "         -0.0122, -0.1286, -0.1066, -0.1421, -0.2869,  0.1265,  0.2813,  0.1080,\n",
      "          0.1238, -0.3376,  0.1359, -0.0410,  0.2352,  0.1185, -0.1682,  0.3894,\n",
      "          0.4837, -0.1884,  0.0923,  0.3109,  0.1763,  0.0841,  0.0832, -0.2112,\n",
      "          0.0484, -0.1428, -0.2887, -0.0050,  0.0670,  0.0890,  0.2170,  0.0736,\n",
      "          0.0262,  0.0147,  0.3572,  0.3137,  0.0837,  0.3432, -0.2798,  0.0383,\n",
      "          0.3950, -0.5202,  0.1438,  0.5015, -0.4562, -0.6651,  0.0707,  0.1524,\n",
      "          0.0379,  0.3060,  0.4334, -0.2978,  0.1729,  0.5553, -0.1821, -0.1045]]), tensor([[ 0.0643, -0.1233,  0.1670, -0.0083,  0.3737,  0.0114, -0.1574, -0.0829,\n",
      "         -0.0848, -0.1920, -0.1029, -0.0367, -0.3347, -0.0631,  0.4439,  0.1288,\n",
      "          0.0839, -0.1391,  0.1688, -0.0889, -0.0923,  0.1780,  0.1029,  0.0891,\n",
      "          0.0895,  0.1983, -0.1704, -0.1576,  0.3401, -0.1080,  0.2319, -0.0226,\n",
      "          0.4397,  0.2855, -0.2488, -0.0244,  0.0340,  0.3298, -0.0196,  0.2378,\n",
      "          0.0087,  0.2019,  0.3342,  0.3577, -0.0425,  0.3142, -0.2374,  0.0430,\n",
      "          0.3832, -0.7915,  0.1203,  0.5958, -0.0368, -0.6937,  0.0708, -0.0752,\n",
      "         -0.1003,  0.0741,  0.1601, -0.0403,  0.0549,  0.4084,  0.0618, -0.4246]]), tensor([[ 0.1333, -0.2380,  0.2432, -0.1037,  0.2536, -0.1007,  0.0402, -0.1329,\n",
      "         -0.1927, -0.1189, -0.0722,  0.0158, -0.2324, -0.2138,  0.3656,  0.0469,\n",
      "          0.1997, -0.0454,  0.2274, -0.0321, -0.1149,  0.1643,  0.2137,  0.1461,\n",
      "          0.2393,  0.3037, -0.1893, -0.0456,  0.3204,  0.1631,  0.4262,  0.0047,\n",
      "          0.1598,  0.2805, -0.2538,  0.1562,  0.1217,  0.1929, -0.0956,  0.0086,\n",
      "         -0.0774,  0.1592,  0.2184,  0.2724,  0.0807,  0.3100, -0.1069,  0.1396,\n",
      "          0.3067, -0.3463,  0.2154,  0.5362, -0.0883, -0.5798,  0.0309, -0.1355,\n",
      "         -0.1039,  0.0343,  0.0863, -0.1978,  0.2423,  0.4817, -0.0689, -0.4193]]), tensor([[ 0.0667, -0.2255,  0.1543, -0.2497,  0.2616, -0.0692,  0.0130,  0.0516,\n",
      "         -0.2334, -0.0481, -0.0095, -0.1007,  0.0945, -0.0454,  0.6291,  0.2079,\n",
      "         -0.1721, -0.0903, -0.1468,  0.0795, -0.0858,  0.0936, -0.0589,  0.6665,\n",
      "          0.3898, -0.0861, -0.1591,  0.2623, -0.1391, -0.3212,  0.6453, -0.0303,\n",
      "          0.0568,  0.1871,  0.0682, -0.1753,  0.1332,  0.3966,  0.3275,  0.0763,\n",
      "         -0.2321, -0.0549,  0.3042,  0.5974,  0.0189,  0.4646, -0.1658,  0.2527,\n",
      "          0.4316, -0.3120,  0.0746,  0.4306,  0.0596, -0.8718,  0.3032,  0.0171,\n",
      "         -0.1970,  0.0820, -0.0271, -0.1998, -0.1647,  0.6962,  0.2056, -0.2620]]), tensor([[ 0.1940, -0.1495,  0.0696,  0.5629, -0.0230,  0.0257,  0.0042, -0.0886,\n",
      "          0.2046, -0.0363,  0.2256,  1.0219, -0.4274, -0.8427,  0.7554, -0.3376,\n",
      "         -0.2966,  0.2121,  0.1851, -0.6830, -0.4522,  0.7434, -0.1217,  0.8003,\n",
      "         -0.2410,  0.1474,  0.0985, -0.2594,  0.4490, -0.0221,  0.3256, -0.1356,\n",
      "          0.3502, -0.0596, -0.1251, -0.5984, -0.2522, -0.1542,  0.2454, -0.2584,\n",
      "         -0.6832, -0.5319,  0.2426,  0.1928,  0.3550,  0.8069, -0.3095,  0.2700,\n",
      "         -0.1978, -0.3885, -0.1911,  0.1239, -0.5952, -1.2121, -0.0350, -0.1997,\n",
      "         -0.6841,  0.6661, -0.0614, -0.7525, -0.4673,  0.1231, -0.2057, -0.2038]]), tensor([[ 0.2637, -0.9360,  0.3110,  0.1300, -0.1119,  0.2953, -0.0964,  0.1080,\n",
      "          0.0841, -0.1270,  0.1003,  0.0746,  0.0977, -0.2499,  0.6497, -0.4319,\n",
      "         -0.1211, -0.1360,  0.3142, -0.2451,  0.1029,  0.3131,  0.4686,  0.3793,\n",
      "          0.3472,  0.0988,  0.0610, -0.1812,  0.7374,  0.3068, -0.0191,  0.0658,\n",
      "         -0.2994,  0.3502, -0.1487,  0.0792,  0.2109,  0.0765,  0.2573,  0.1351,\n",
      "          0.1259,  0.1985, -0.2213,  0.0185, -0.5495,  0.2148, -0.1798, -0.4437,\n",
      "          0.5860,  0.1078,  0.2571,  0.7745, -0.0184, -0.8873,  0.3879, -0.0914,\n",
      "         -0.3790,  0.3453, -0.2075, -0.3014,  0.6986,  0.2523,  0.4460, -0.1442]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_DSet, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_DSet, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256c456",
   "metadata": {},
   "source": [
    "## Create and Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a981bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "LitNeuralNetwork(\n",
      "  (ff): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=48, bias=False)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (rnn): RNN(48, 16, bias=False)\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=6, bias=False)\n",
      "    (1): LogSoftmax(dim=1)\n",
      "  )\n",
      "  (loss_fn): NLLLoss()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | ff      | Sequential | 3.1 K  | train\n",
      "1 | rnn     | RNN        | 1.0 K  | train\n",
      "2 | out     | Sequential | 96     | train\n",
      "3 | loss_fn | NLLLoss    | 0      | train\n",
      "-----------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                               | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liamr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\Liamr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a90ab135344a85ab6e5ca0262d95b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class LitNeuralNetwork(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        \"\"\" Builds recurrent neural network model \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # build layers\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(64, 48, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.rnn = nn.RNN(48, 16, nonlinearity='relu', bias=False)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(16, 6, bias=False),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        \n",
    "        # build mask for recurrent layer (hh weights -- no self-connect)\n",
    "        self.mask = torch.ones(16, 16).to(device)\n",
    "        dia_ind = np.diag_indices(self.mask.shape[0])\n",
    "        self.mask[dia_ind[0], dia_ind[1]] = torch.zeros(self.mask.shape[0]).to(device)\n",
    "        \n",
    "        # set loss function\n",
    "        self.loss_fn = nn.NLLLoss()\n",
    "\n",
    "    def forward(self, q):\n",
    "        \"\"\" Implements feed-forward then recurrent layer \"\"\"\n",
    "        ff_q = []\n",
    "        for word in q:\n",
    "            ff_q.append(self.ff(word))\n",
    "        ff_q = torch.stack(ff_q)\n",
    "        h_N = Variable(torch.zeros(1, 16)).to(device)\n",
    "        rnn_out = Variable(torch.zeros(1, 16)).to(device)\n",
    "        self.rnn._parameters['weight_hh_l0'].data.mul_(self.mask)\n",
    "        for word in ff_q:\n",
    "            if not torch.all(word.eq(0)):\n",
    "                #word = word / word.sum().item() # normalize\n",
    "                word = word / torch.max(word) # normalize\n",
    "                rnn_out, h_N = self.rnn(word, h_N)\n",
    "            else:\n",
    "                break\n",
    "        output = self.out(rnn_out)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # perform training\n",
    "        X, y = batch\n",
    "        X = torch.stack(X)\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        # Log learning rate\n",
    "        lr = self.optimizers().param_groups[0]['lr']\n",
    "        self.log('learning_rate', lr, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # perform validation (using test dataset)\n",
    "        X, y = batch\n",
    "        X = torch.stack(X)\n",
    "        out = self(X)\n",
    "        loss = self.loss_fn(out, y)\n",
    "\n",
    "        # calculate acc\n",
    "        labels_hat = torch.argmax(out, dim=1)\n",
    "        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
    "\n",
    "        # log the outputs\n",
    "        self.log_dict({'val_loss': loss, 'val_acc': val_acc})\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "# Define a model checkpoint callback to save the model every 5 epochs\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=-1,               # Save all checkpoints (set to 1 to save only the best one)\n",
    "    every_n_epochs=5,            # Save every 5 epochs\n",
    "    filename='{epoch}-{val_loss:.2f}',  # Format to save the file name with epoch and validation loss\n",
    "    verbose=True                 # Print information when saving\n",
    ")\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# create model\n",
    "model = LitNeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "# use lightning for training\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20,            # Number of epochs\n",
    "    devices=1,                # Number of devices (GPUs/CPUs)\n",
    "    accelerator='auto',       # Automatically select the device\n",
    "    precision=32,             # Use mixed precision for faster training\n",
    "    gradient_clip_val=0.5,    # Clip gradients -- unnecessary?\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f51ab5",
   "metadata": {},
   "source": [
    "## Check Model and Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "873219a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 2 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 2 -- Inferred: 2\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 2 -- Inferred: 2\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 2 -- Inferred: 2\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 2\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 2 -- Inferred: 2\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 2\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 2 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 2 -- Inferred: 2\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 2 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 2 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 4 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 1 -- Inferred: 3\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 5 -- Inferred: 5\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 0 -- Inferred: 0\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 4 -- Inferred: 4\n",
      "Actual: 3 -- Inferred: 3\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 1 -- Inferred: 1\n",
      "Actual: 5 -- Inferred: 0\n",
      "Actual: 5 -- Inferred: 0\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 1.013425 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check performance on test dataset\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = torch.stack(X).to(device), y.to(device) #torch.FloatTensor(y).to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += model.loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            print(\"Actual: {0} -- Inferred: {1}\".format(y.item(), pred.argmax(1).item()))\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "model.to(device)\n",
    "test(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834f5850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | ff      | Sequential | 3.1 K  | train\n",
      "1 | rnn     | RNN        | 1.0 K  | train\n",
      "2 | out     | Sequential | 96     | train\n",
      "3 | loss_fn | NLLLoss    | 0      | train\n",
      "-----------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                               | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liamr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\Liamr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de01d90117234b3b962e647487d15d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                    | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# load model from checkpoint (lightning has automatic, most-recent-epoch checkpointing) and continue training\n",
    "\n",
    "model = LitNeuralNetwork.load_from_checkpoint(cwd+\"\\\\lightning_logs\\\\version_1\\\\checkpoints\\\\epoch=19-val_loss=1.01.ckpt\")\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b18c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "torch.Size([48, 64])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3191, -0.1007,  0.2364,  ...,  0.0308, -0.6877, -0.3279],\n",
      "        [-0.3180, -0.0725,  0.1145,  ...,  0.0960, -0.7081, -0.1855],\n",
      "        [ 0.2543,  0.0214,  0.3525,  ...,  0.0748, -0.0897,  0.1017],\n",
      "        ...,\n",
      "        [ 0.5771, -0.0987, -0.1120,  ..., -0.0190, -0.4856, -0.4656],\n",
      "        [ 0.1736, -0.1623,  0.1439,  ...,  0.1375, -0.3094,  0.2778],\n",
      "        [-0.5711, -0.0402, -0.2456,  ..., -0.1565,  0.0552, -0.5107]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Layer 2\n",
      "torch.Size([16, 48])\n",
      "Parameter containing:\n",
      "tensor([[ 0.6916,  0.6156,  0.8338,  0.7775,  0.1623,  0.7049,  0.6421,  0.1841,\n",
      "          0.7779,  0.7551,  0.9258,  0.5336,  0.6808,  0.9846,  0.5271,  0.5804,\n",
      "          0.9415,  0.5120, -0.0985,  0.8340,  0.7431,  0.7384,  0.7556,  1.1425,\n",
      "          0.6219,  0.2811,  0.9587,  0.9397,  0.7397,  0.8261,  0.9777,  0.1242,\n",
      "          0.9205, -0.7322, -0.1207,  1.2899,  0.5577,  1.1331,  0.2080,  0.2077,\n",
      "          0.7524,  0.6076, -0.7634,  0.4677,  2.0513,  0.4366,  0.5623,  0.5919],\n",
      "        [ 0.4972,  0.3517,  0.7541, -1.0523, -0.3617, -0.6983,  0.4296, -0.0952,\n",
      "          0.5083,  0.7544,  0.1171, -0.4475, -1.1073, -0.3220,  0.4865,  0.4419,\n",
      "          0.1523,  0.2952, -0.0787, -0.7807, -1.0068, -0.7942,  0.1737, -1.0337,\n",
      "          0.5357,  0.7138, -0.4169,  0.6515,  0.3534, -0.1920, -0.2703,  0.1694,\n",
      "         -0.3488, -0.5791,  0.4547, -1.2066, -0.8382, -0.6038,  0.6222,  0.4691,\n",
      "         -0.8267,  0.6292,  0.3204,  0.4190, -1.5595,  0.6596,  0.5793, -0.5362],\n",
      "        [ 0.2687,  0.1992, -0.2806, -0.4480,  0.1420,  0.8650, -0.0553,  0.4419,\n",
      "         -0.1490,  0.2213,  0.4877,  0.6679,  0.7194,  0.2323, -0.0160, -0.0880,\n",
      "          0.4580,  0.3204, -0.0261,  0.5402,  1.2370,  0.4328,  0.4335,  2.1087,\n",
      "          0.2571, -0.3798,  0.3450, -0.1376,  0.2929, -0.0780,  0.6570, -0.4458,\n",
      "          0.2774,  0.4359,  0.1926,  1.1338,  0.0853,  0.6935, -0.2722, -0.7545,\n",
      "          0.3173, -0.2053, -0.4521,  0.0086,  2.1989,  0.3349, -0.1631,  0.5637],\n",
      "        [ 0.4603,  0.3911, -0.3719, -1.1260,  0.6649,  0.7045, -0.0616,  0.7478,\n",
      "         -0.2740,  0.0515,  0.5511,  0.5350,  0.6145,  0.6471,  0.2749, -0.1596,\n",
      "          0.4445,  0.7250,  0.3409,  0.6490,  1.3720,  0.6583,  0.1965,  2.5156,\n",
      "          0.3040, -0.2565,  0.5391, -0.1956,  0.0074,  0.5351,  0.8525, -1.1734,\n",
      "          0.5077,  0.2774,  0.6559,  0.6833, -0.4483,  0.7333,  0.2170, -0.2639,\n",
      "          0.7559, -0.2759, -0.4556,  0.3950,  2.1094,  0.1236, -0.1255,  0.7463],\n",
      "        [ 0.4363,  0.5473,  0.5022,  1.0854,  0.0600, -0.0805,  0.3296,  0.0873,\n",
      "          0.5517,  0.2613,  0.5066,  0.1636, -0.1771, -0.1264,  0.5757,  0.4617,\n",
      "          0.0934,  0.4569, -0.4496, -0.2012, -0.8070, -0.1672,  0.4448, -1.2989,\n",
      "          0.4089,  0.3236,  0.2321,  0.3685,  0.5590,  0.5922, -0.3938,  0.6681,\n",
      "          0.1015,  0.0090,  0.5884, -0.5037,  0.3291,  0.1331,  0.5626,  0.5966,\n",
      "         -0.0901,  0.7460,  0.5696,  0.4159, -1.5706,  0.3477,  0.6292, -0.2037],\n",
      "        [ 0.3598,  0.3255,  0.1575, -0.8362, -1.1120,  0.0308,  0.3420, -1.0532,\n",
      "          0.4343,  1.0121,  0.8934,  0.4993, -0.0520,  0.4112, -0.0923,  0.8066,\n",
      "          1.4388,  0.7631, -1.8172,  0.2051, -0.3829, -0.3224,  0.5438,  0.6303,\n",
      "          0.6485, -0.2452,  1.5433,  0.6611,  0.3026,  0.7147, -0.0208, -1.0833,\n",
      "          0.7297,  0.5078,  0.4297,  0.1565, -0.5569,  0.5342,  0.0416, -0.7335,\n",
      "          0.7793,  0.6097, -1.5205,  0.6432,  0.0226,  0.8107,  0.2977,  0.4060],\n",
      "        [ 0.4138,  0.4797,  0.5434,  1.7964,  0.1882, -0.1799,  0.3799,  0.0478,\n",
      "          0.6324,  0.2278,  0.2770, -0.2104, -0.1754,  0.1367,  0.1861,  0.1041,\n",
      "         -0.1472, -0.1056, -0.0034,  0.2654,  0.2880,  0.0776,  0.0326, -1.2385,\n",
      "          0.0423,  0.6370, -0.0764,  0.5987,  0.2088,  0.0952, -0.5241,  1.0907,\n",
      "          0.1901, -0.1479, -0.0181, -0.3264,  0.6806,  0.3047,  0.5297,  0.7764,\n",
      "         -0.2860,  0.4424,  1.1834,  0.4868, -0.3665,  0.3216,  0.4241,  0.0361],\n",
      "        [ 0.5249,  0.8846,  0.5689,  2.1193, -0.0900,  0.0459,  0.6675, -0.1712,\n",
      "          1.0079,  0.5643,  0.5650, -0.3487, -0.3204,  0.0433,  0.4990,  0.6854,\n",
      "         -0.1034,  0.1677,  0.2822,  0.0401, -0.5336, -0.1632,  0.7106, -1.5071,\n",
      "          0.4452,  0.7072,  0.2430,  0.5777,  0.7107,  0.3371, -0.1353,  0.6378,\n",
      "          0.3322, -0.5992,  0.6667,  0.5940,  0.7923, -0.0725,  0.7475,  0.6536,\n",
      "         -0.3538,  0.7788,  1.0779,  0.4840, -1.2923,  0.3038,  0.6337, -0.2805],\n",
      "        [ 0.8447,  1.0644,  0.5960, -2.0190, -0.5564,  0.0906,  0.5084, -0.3148,\n",
      "         -0.1386,  0.8694,  0.8210,  0.3606, -0.9358, -0.6015,  0.9348,  0.7590,\n",
      "          1.2131,  1.3456, -0.4961, -0.5292, -1.3610, -0.0520,  1.0140,  0.0571,\n",
      "          0.9467, -0.3659,  1.1566, -0.0076,  0.7549,  1.1875, -0.3119, -2.3216,\n",
      "          0.3092, -0.0331,  2.1237,  0.5186, -1.9631, -0.0115,  0.2401, -0.1929,\n",
      "          0.6051,  1.0533, -0.0930,  0.3813, -2.2246,  0.7332,  0.3640, -0.0275],\n",
      "        [ 0.4813,  0.6619,  0.8083, -0.3974, -0.0506,  0.2653,  0.4853,  0.0072,\n",
      "          0.2661,  0.5453,  0.5521, -0.0837, -0.4430,  0.0137,  0.8790,  0.5312,\n",
      "          0.2805,  0.6199, -0.1417,  0.0463, -1.4259,  0.1359,  0.6566, -1.3381,\n",
      "          0.6100,  0.3846,  0.1135,  0.5222,  0.4698,  0.7407,  0.1182, -0.0063,\n",
      "          0.4267, -0.4120,  1.5459, -0.1518, -0.1664, -0.0832,  0.6293,  0.6798,\n",
      "          0.1441,  0.8446,  0.4580,  0.2935, -2.5926,  0.5025,  0.5402, -0.0689],\n",
      "        [ 0.0625,  0.3608, -0.0052,  0.2058,  0.9111, -0.0513,  0.2895,  1.1147,\n",
      "         -0.0267, -0.1775, -0.5657, -0.5112,  0.3652, -0.4322,  0.8032, -0.3675,\n",
      "         -0.5780,  0.1477,  1.1667, -0.3618, -0.0314,  0.1207,  0.4097, -0.5524,\n",
      "          0.2077,  0.7049, -0.7652, -0.0337, -0.1487,  0.3715,  0.2161,  0.3729,\n",
      "         -0.2525, -1.0650,  0.6304, -1.4561, -0.3260, -0.9847,  0.6414,  0.6571,\n",
      "         -0.3288, -0.0167,  2.2091,  0.0922, -1.4645,  0.1138,  0.5854,  0.0249],\n",
      "        [ 0.6581,  0.1279,  0.5442, -1.2331,  0.7435, -0.1532,  0.2143,  0.7364,\n",
      "          0.0158,  0.3369, -0.6675, -0.2878, -0.2514, -0.3554,  0.6885,  0.1696,\n",
      "         -0.1218,  0.3899,  0.2809, -0.1781, -0.3614, -0.1901,  0.6275, -0.3817,\n",
      "          0.4343,  1.0059, -0.8001,  0.2119,  0.3326,  0.4033,  0.0186, -1.1142,\n",
      "         -0.3543, -1.1259,  0.5888, -0.8026, -0.9211, -0.8957,  0.5710,  0.9311,\n",
      "         -0.3497,  0.1063,  1.1571,  0.1345, -1.9320,  0.3261,  0.3711, -0.2375],\n",
      "        [-0.2452, -0.1628, -0.0530,  1.0656,  0.3488,  0.7146, -0.3061,  0.6680,\n",
      "          0.1931, -0.3357,  0.3032,  0.5114,  0.4822,  0.7501, -0.3403,  0.0529,\n",
      "         -0.3232, -0.1753,  0.4730,  0.6422,  1.7995,  0.3667, -0.3131,  1.3242,\n",
      "         -0.5835, -0.0782,  0.0121,  0.3369, -0.0386, -0.5196,  0.4385,  0.5257,\n",
      "          0.2894, -0.5751, -1.6309,  0.3576,  0.8800,  0.4090, -0.2919, -0.3298,\n",
      "          0.1802, -0.1511, -1.1016, -0.1404,  3.3646, -0.3886, -0.2716,  0.6945],\n",
      "        [-0.0660, -0.1781, -0.2465,  0.0683,  0.1066, -0.1087, -0.2179, -0.1585,\n",
      "          0.1096, -0.1614,  0.1209, -0.1816, -0.1721,  0.0485, -0.1860, -0.1094,\n",
      "         -0.2078, -0.2355,  0.0449, -0.0303,  0.0597, -0.3338,  0.0591, -0.2473,\n",
      "         -0.2345, -0.0964,  0.0065,  0.1087, -0.3060, -0.0457,  0.0850, -0.0975,\n",
      "          0.0893, -0.0891, -0.0636, -0.1797, -0.0231,  0.1041,  0.0831, -0.1055,\n",
      "          0.0439,  0.1194, -0.0829, -0.0429, -0.3123, -0.3707, -0.2911, -0.0037],\n",
      "        [ 0.7892,  0.6866,  0.4502, -1.5571, -0.0188,  0.0570,  0.2211,  0.1898,\n",
      "         -0.3967,  0.4959, -0.0320,  0.3205, -0.4799, -0.7005,  0.9760,  0.1660,\n",
      "          0.7793,  1.1081,  0.0386, -0.8894, -1.3117,  0.3028,  0.8762, -0.0360,\n",
      "          1.0757, -0.0453,  0.7619, -0.4888,  0.6169,  0.9653, -0.1999, -2.1936,\n",
      "          0.2816, -0.3091,  2.1041,  0.3340, -1.6510, -0.0066,  0.0494,  0.1357,\n",
      "          0.4802,  0.9963,  0.5502,  0.4088, -2.3176,  0.5701,  0.0479, -0.0480],\n",
      "        [ 0.1954,  0.2603,  0.6796, -0.1442, -0.4335, -0.2251,  0.0953, -0.0643,\n",
      "         -0.0802,  0.4259,  0.1442,  0.1072, -0.4915, -0.1585,  0.3142,  0.3214,\n",
      "          0.1704,  0.2082, -0.5259, -0.4627, -1.0924, -0.4892,  0.1792, -1.5232,\n",
      "          0.2080,  0.6645, -0.2756,  0.0066,  0.1496,  0.0656, -0.4546,  0.2819,\n",
      "         -0.2564,  0.0227,  0.5580, -1.3262, -0.7283, -0.4214,  0.1416,  0.4758,\n",
      "          0.0076,  0.3724,  0.2930, -0.0455, -1.8423,  0.1376,  0.4403, -0.0911]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Layer 3\n",
      "torch.Size([16, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.0000e+00, -1.0377e+00,  2.3846e-02,  9.4021e-02,  2.9968e-01,\n",
      "          1.4238e-02, -2.9226e-01, -1.1407e-02, -2.6718e-01,  7.9075e-02,\n",
      "         -1.7797e+00, -9.0429e-01,  5.9524e-02,  1.0517e-01, -4.5226e-03,\n",
      "         -8.6483e-02],\n",
      "        [-4.5995e-01,  0.0000e+00, -3.8383e-01, -6.7208e-02,  2.6763e-01,\n",
      "          5.9080e-01, -1.8349e-01,  8.5152e-02,  3.7571e-01,  1.3196e-01,\n",
      "          1.6437e-01,  3.5409e-01,  1.0191e-01, -3.1617e-02,  2.0936e-02,\n",
      "          8.9755e-01],\n",
      "        [ 3.0415e-01, -2.5573e-01,  0.0000e+00,  3.9402e-01, -1.2623e-01,\n",
      "          2.3574e-01, -6.1834e-01, -1.9086e-01,  2.8238e-01,  5.0316e-02,\n",
      "         -5.9780e-02, -1.5149e-01,  1.4962e-01,  1.0714e-01,  3.9623e-01,\n",
      "         -1.0267e-01],\n",
      "        [ 5.7463e-01, -6.3698e-02,  4.2139e-01, -0.0000e+00, -2.0881e-01,\n",
      "          4.2899e-04, -4.8462e-01, -7.3734e-01,  4.5869e-01,  3.4659e-02,\n",
      "          1.2066e-01,  2.9641e-01, -1.4648e-01, -9.4412e-02,  2.3167e-01,\n",
      "         -2.0529e-01],\n",
      "        [ 3.4842e-01,  1.1787e-01, -2.8515e-01, -4.0887e-01, -0.0000e+00,\n",
      "          8.0749e-02,  6.9986e-01,  7.3655e-01,  6.3457e-02,  5.2375e-01,\n",
      "         -4.7027e-01, -2.4488e-01,  7.0291e-02, -1.6032e-01,  3.1542e-01,\n",
      "          3.5943e-01],\n",
      "        [ 5.8338e-01,  1.3091e-01,  3.6854e-01,  8.4670e-02,  2.5541e-01,\n",
      "          0.0000e+00, -6.5448e-01,  2.5321e-01,  5.6143e-01,  3.2617e-01,\n",
      "         -1.2134e+00, -7.8983e-01, -3.1739e-01,  9.5842e-02,  1.5163e-01,\n",
      "          1.9577e-01],\n",
      "        [-2.0458e-01, -7.6976e-02, -5.1323e-01, -1.1323e+00,  5.2538e-01,\n",
      "          1.7280e-01, -0.0000e+00,  6.3547e-01, -1.2270e-01,  2.5292e-01,\n",
      "          2.7224e-02,  3.4544e-02, -7.3431e-01, -4.9376e-02, -1.9770e-02,\n",
      "          3.2381e-01],\n",
      "        [ 1.3172e-01, -3.0292e-01, -2.3051e-01, -2.3180e-01,  3.7562e-01,\n",
      "         -4.2336e-01,  9.6236e-01, -0.0000e+00, -4.9242e-01,  1.9621e-02,\n",
      "          2.9553e-01,  5.1415e-02,  3.1299e-02, -4.4639e-02, -2.0208e-01,\n",
      "         -2.2694e-01],\n",
      "        [-1.0549e-01, -1.7684e-01,  1.0651e-01,  1.6545e-01,  4.5020e-01,\n",
      "          5.2600e-01, -4.6624e-01, -9.5016e-03,  0.0000e+00,  7.7353e-01,\n",
      "         -7.6246e-02,  2.7620e-02, -1.0272e+00, -1.2833e-01,  5.6779e-01,\n",
      "          8.1032e-02],\n",
      "        [ 2.7992e-01, -3.8415e-01, -4.7860e-01, -1.4122e-01,  3.9333e-01,\n",
      "         -2.7645e-02,  6.6986e-01,  1.2232e+00,  1.9455e-01,  0.0000e+00,\n",
      "         -5.3441e-01, -4.1032e-02, -2.0378e-03, -1.5093e-02, -2.9073e-02,\n",
      "         -1.9948e-01],\n",
      "        [-2.8288e-01,  3.8562e-02, -3.1641e-01,  2.3887e-01,  2.7789e-01,\n",
      "         -4.6915e-01,  4.7987e-01,  2.2264e-01,  3.0913e-01,  1.8161e-01,\n",
      "         -0.0000e+00,  8.1407e-01, -4.8274e-01,  2.3487e-01,  5.2599e-01,\n",
      "          3.4238e-02],\n",
      "        [-3.5287e-01,  7.4066e-02, -1.5848e-01, -3.3402e-01,  4.7319e-01,\n",
      "         -2.4836e-01, -1.5840e-01,  5.0288e-01,  5.3761e-02,  4.3760e-01,\n",
      "          5.5479e-01,  0.0000e+00, -3.5433e-01,  1.1947e-01,  5.4719e-01,\n",
      "          3.8291e-01],\n",
      "        [ 7.4523e-01, -1.0549e+00,  4.5600e-01,  2.2312e-01, -5.2675e-02,\n",
      "          1.5884e-01, -2.6067e-01, -1.7295e-01, -7.6495e-01,  3.7412e-01,\n",
      "         -7.6096e-01, -5.7771e-01, -0.0000e+00,  1.6979e-01, -3.8990e-01,\n",
      "          9.7694e-02],\n",
      "        [-1.3918e-01, -1.5976e-01, -6.0254e-02,  7.4899e-02, -4.3470e-02,\n",
      "         -9.4281e-02,  1.9525e-02, -1.6518e-01, -1.4008e-01,  8.2950e-02,\n",
      "         -1.3663e-01, -5.4668e-02, -2.5174e-01, -0.0000e+00, -2.4208e-01,\n",
      "          8.2730e-02],\n",
      "        [ 3.3165e-02, -2.1754e-01, -7.3286e-02,  1.7561e-01,  3.0496e-01,\n",
      "         -2.5394e-02, -4.4036e-01,  4.5585e-01,  6.0579e-01,  7.6583e-01,\n",
      "          1.4977e-01,  2.1212e-01, -7.2118e-01, -2.2450e-01, -0.0000e+00,\n",
      "         -5.1603e-01],\n",
      "        [ 3.3154e-01,  5.8594e-01, -4.5978e-01, -3.8967e-02,  6.4365e-01,\n",
      "         -1.3054e-03,  4.9355e-01,  7.5669e-01, -4.6817e-02,  4.6771e-01,\n",
      "         -2.9939e-02,  2.5844e-01,  1.1032e-01,  1.4753e-01, -2.1160e-01,\n",
      "         -0.0000e+00]], device='cuda:0', requires_grad=True)\n",
      "Layer 4\n",
      "torch.Size([6, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.7060,  0.2839, -0.0584, -0.3811,  0.4804,  1.2993, -1.5369, -0.4208,\n",
      "          0.4929,  0.1045, -0.5487, -0.1752, -0.9634, -0.0814,  0.1583,  0.0708],\n",
      "        [-0.6566,  0.5510, -0.9432, -0.6828, -1.0205, -0.6833, -1.2311, -1.7416,\n",
      "         -0.0170, -1.7026,  0.0500,  0.2923,  0.0464, -0.0795,  0.0981,  0.5715],\n",
      "        [-0.5943, -0.0442, -0.2214, -0.6808,  0.2433,  0.0732, -1.1147, -1.4471,\n",
      "         -0.0831, -0.2102, -0.8033, -0.2989, -1.1767,  0.1322, -0.5916, -0.1450],\n",
      "        [-1.5191, -0.0476, -0.4441, -0.9941,  0.1267, -1.3346,  1.6709,  1.2748,\n",
      "         -0.1816,  0.6094,  0.0095,  0.3683, -1.1787,  0.1059,  0.1668, -0.3848],\n",
      "        [ 1.6382, -0.3507,  0.4521,  0.6013, -2.5322, -0.3685,  1.0303, -1.1390,\n",
      "         -1.7810, -2.6649, -0.1551, -0.2812,  1.5671, -0.0243, -1.6211, -0.7545],\n",
      "        [-0.8540, -0.8232,  0.4531,  0.5450, -0.8837, -2.5738,  1.3189, -1.2942,\n",
      "          0.1477, -1.3007,  0.5491, -0.1019, -0.8417, -0.0073,  0.4774, -1.1448]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# check all weights\n",
    "\n",
    "param_list = [*model.parameters()]\n",
    "i = 0\n",
    "for lay in param_list:\n",
    "    i += 1\n",
    "    print(\"Layer {0}\".format(i))\n",
    "    print(lay.shape)\n",
    "    print(lay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0c209",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "314011da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to Recurrent ANN Models//RANN_2.pth\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"Recurrent ANN Models//\"\n",
    "model_name = \"RANN_2.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_dir + model_name)\n",
    "print(\"Saved PyTorch Model State to \" + model_dir + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d75a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207de82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
