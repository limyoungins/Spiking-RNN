{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43712388-c442-4021-bc0f-e898d4323078",
   "metadata": {},
   "source": [
    "# Analogous Recurrent ANN Trainer\n",
    "\n",
    "This notebook can be used to generate and train a recurrent ANN so that the weights can be copied over to a SNN with the same architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bf3b9-a1df-4af0-8122-1af8a12c46d8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf949556-79d1-4121-b739-50c0ca083181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af0fa6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502bd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data and prepare for vector conversion\n",
    "\n",
    "# load data\n",
    "f = open(\"Training Data\\\\train_5500.txt\")\n",
    "data = f.read()\n",
    "\n",
    "# split data into sentences\n",
    "sents = data.split('\\n')\n",
    "\n",
    "# split each sentence into words\n",
    "for i in range(len(sents)):\n",
    "    sents[i] = sents[i].split(' ')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402e0cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liamr\\AppData\\Local\\Temp\\ipykernel_3736\\742923494.py:14: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  vecs.append(torch.from_numpy(wv[word]))\n"
     ]
    }
   ],
   "source": [
    "# perform word to vector conversion\n",
    "\n",
    "# load in word to vector converter\n",
    "w2v_load = Word2Vec.load(\"Usable Word2Vec Model\\\\word2vec.model\")\n",
    "wv = w2v_load.wv\n",
    "\n",
    "# perform conversion\n",
    "vec_data = []\n",
    "for sent in sents[:-1]:\n",
    "    vecs = []\n",
    "    vecs.append(sent[0])\n",
    "    for word in sent[1:]:\n",
    "        try:\n",
    "            vecs.append(torch.from_numpy(wv[word]))\n",
    "        except:\n",
    "            pass\n",
    "    vecs.append(torch.zeros(64))\n",
    "    vec_data.append(vecs)\n",
    "\n",
    "# pad all sentences to length of longest sentence\n",
    "max_len = max([len(sent) for sent in vec_data])\n",
    "vec_data_pad = []\n",
    "for sent in vec_data:\n",
    "    pad_len = max_len - len(sent)\n",
    "    for i in range(pad_len):\n",
    "        sent.append(torch.zeros(64))\n",
    "    vec_data_pad.append(sent)\n",
    "vec_data = vec_data_pad\n",
    "\n",
    "# split into training and test data\n",
    "train_data = vec_data[:5000]\n",
    "test_data = vec_data[5000:-1]\n",
    "\n",
    "# NOTE: first word of each sentence is correct categ. -- last sentence is empty (excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b78ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataSet which can be used with PyTorch DataLoader\n",
    "\n",
    "ans_key = { 'DESC' :  0,\n",
    "            'ENTY' :  1,\n",
    "            'ABBR' :  2,\n",
    "            'HUM'  :  3,\n",
    "            'LOC'  :  4,\n",
    "            'NUM'  :  5 }\n",
    "\n",
    "class QuestionDataset(Dataset):\n",
    "    \"\"\" Question Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        data = list of (list of words -- first word is label)\n",
    "        \"\"\"\n",
    "        self.labels = []\n",
    "        self.sents = []\n",
    "        for sent in data:\n",
    "            lab_val = ans_key[sent[0].split(\":\")[0]]\n",
    "            lab_arr = torch.tensor(lab_val)\n",
    "            self.labels.append(lab_arr)\n",
    "            self.sents.append(sent[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sent = self.sents[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sent, label\n",
    "\n",
    "train_DSet = QuestionDataset(train_data)\n",
    "test_DSet = QuestionDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482a8daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.7501, -0.3085, -0.3712,  1.2873, -0.3685, -0.1576,  0.6909, -0.1197,\n",
      "         -1.3665, -0.5399,  0.5410,  0.3175, -0.8711, -1.6558, -0.6625,  1.4596,\n",
      "         -0.4190, -0.1719, -0.4393,  0.9215,  0.7218,  0.0266,  1.7388, -0.4094,\n",
      "          0.4647,  2.1667,  0.4310, -0.7656, -0.4223, -1.2149, -0.5025, -1.2658,\n",
      "         -2.3934,  0.1058, -1.4723,  0.7023,  0.6207, -1.3136, -0.9164,  0.7105,\n",
      "         -0.5051,  0.5171,  0.5749, -1.5660,  2.4641, -1.1979,  0.6296, -0.5382,\n",
      "          0.9257, -0.2696,  0.7026,  0.8137,  0.6088,  3.1219, -0.6065,  0.5419,\n",
      "          0.7573, -0.5837,  0.0873,  0.8292,  0.5570,  0.4734,  0.3817,  2.1990]]), tensor([[-0.1187,  0.4110,  0.4454,  0.4726, -0.5734, -0.6245,  1.2417, -0.2847,\n",
      "         -0.9390, -0.3459,  0.2911,  0.8602, -1.7420, -0.7528, -0.7632,  1.7993,\n",
      "          1.9305, -0.6733,  0.1016,  0.8474,  1.0793,  0.7145, -0.0355, -1.8987,\n",
      "          0.5883,  1.7181, -1.1460,  0.1994, -0.3177, -2.5541,  1.2290, -0.7425,\n",
      "         -2.8377,  0.8054, -1.5003,  0.4846,  0.3475, -2.4495, -0.1332,  0.1769,\n",
      "         -0.9392,  0.9142,  0.7202, -0.1680,  1.6159,  0.2431,  0.0568, -0.2793,\n",
      "         -1.0431, -0.7256, -0.2582, -0.4307,  1.1389,  0.6078, -0.9176, -0.0572,\n",
      "         -0.9448, -1.2399,  0.0771,  0.9165, -0.5416, -0.0393, -0.6065,  0.4235]]), tensor([[-0.6285, -0.4285, -1.5015,  1.4464,  0.4465, -1.7007,  0.6083, -0.9673,\n",
      "         -1.5438, -1.4996, -0.3953,  0.1827, -0.4493, -1.0028, -2.1414,  1.3168,\n",
      "          1.4676,  0.2067,  1.6426,  1.1731,  0.5019,  0.6255, -0.1725, -0.6669,\n",
      "          2.1950,  2.1434, -0.7876, -2.6138, -1.2381, -1.5200,  2.0050, -0.6826,\n",
      "         -3.0280,  1.4432,  0.1908,  0.0398, -0.0948, -2.3297,  0.9542,  0.3284,\n",
      "         -0.6865,  0.2745, -0.3726, -0.3000,  1.9443, -0.5690, -0.8963, -0.1017,\n",
      "         -0.3461, -1.4359, -0.6754,  0.2441,  0.7605,  0.4510, -0.4227,  1.8055,\n",
      "         -0.1499, -2.2785, -0.5538,  0.8799, -0.3829, -0.2380,  0.5274,  0.1698]]), tensor([[ 1.7433e+00, -1.0795e+00, -4.8237e-01,  1.4639e+00, -1.1971e+00,\n",
      "         -1.8441e+00,  1.5803e+00,  2.1578e+00, -1.2484e+00, -4.4521e-01,\n",
      "         -1.5168e-03, -3.1351e-01, -2.1644e+00, -1.9175e+00, -2.3539e+00,\n",
      "          1.1648e+00,  1.4203e+00, -9.9259e-01, -1.0841e+00,  7.5463e-01,\n",
      "          1.1910e+00,  1.6567e+00,  1.0535e+00, -6.4528e-02,  3.1721e-01,\n",
      "          3.0399e-01, -7.7175e-01, -3.0071e+00, -4.5354e-01, -3.6204e-01,\n",
      "         -4.2646e-01,  4.0061e-01, -1.5918e-01, -1.0377e-01, -9.9669e-01,\n",
      "          8.0129e-02, -8.7969e-01, -1.8032e+00,  1.9407e+00, -1.4772e+00,\n",
      "         -1.4616e+00,  1.3214e+00,  6.9126e-01,  1.4130e+00, -5.8464e-01,\n",
      "         -7.7223e-01, -2.2458e+00,  1.2685e+00,  1.1830e+00,  2.0494e-01,\n",
      "          2.2801e-01, -1.7268e-01,  2.0329e+00,  5.0388e-01, -2.3871e+00,\n",
      "         -6.6649e-01, -1.0329e+00, -1.3661e+00,  1.0871e+00,  1.5274e+00,\n",
      "         -2.7806e-01,  7.7979e-01,  1.5561e+00, -1.8907e-01]]), tensor([[ 0.7914, -0.4337,  0.4840,  0.8412, -0.2769, -0.8114,  0.6583,  0.7997,\n",
      "         -0.7008, -0.1941,  0.0538, -0.0270, -0.8838, -0.3044, -0.3249, -0.5329,\n",
      "          0.1122,  0.3155, -0.8432,  0.3613,  0.4751,  0.5641,  0.1171,  0.0835,\n",
      "          0.1542,  1.2621, -1.0119, -0.0840, -0.1723,  0.2437, -0.6063,  0.0637,\n",
      "         -0.9577, -0.1626, -0.4114,  0.5020, -0.1956, -0.8559,  1.2147,  0.1420,\n",
      "         -1.0824,  1.2365, -0.2457,  0.2213,  0.6782, -0.7013, -0.8606, -0.3203,\n",
      "         -0.1827, -0.3845, -0.5225, -0.1358,  0.7025,  1.1886,  0.7822, -0.3219,\n",
      "         -0.6391, -1.4577, -0.2779,  1.0198, -1.0825, -0.5857, -0.8527, -0.3185]]), tensor([[-0.5685, -2.1666,  1.0948,  2.9196, -1.0868, -1.9550,  2.0049,  0.2896,\n",
      "         -0.6051, -2.1055,  1.0131,  1.3470, -0.8869, -1.0002, -0.7910, -0.9879,\n",
      "          0.4690,  0.9351, -1.7427, -1.1977,  0.6317,  1.4691, -0.0225,  0.0058,\n",
      "          1.7513,  0.6384, -0.3164, -0.2046, -0.3625, -1.4862, -1.6758,  0.0642,\n",
      "          0.1877, -0.2470, -1.5156,  0.5005,  0.1788, -1.8957,  0.8589,  0.3213,\n",
      "         -2.0326,  1.1033, -0.4600, -0.5406,  1.2488, -0.2288, -2.4499, -0.0762,\n",
      "          0.7648, -0.8954, -1.0448,  0.6403,  1.0303,  0.8589, -1.1415, -1.7132,\n",
      "         -0.7450, -1.3209, -0.4571,  1.7723, -2.0530, -0.1502, -1.5439, -0.5693]]), tensor([[ 0.3039, -0.1216,  0.5077,  0.6637, -0.2148, -1.0219,  0.4546,  0.8906,\n",
      "         -0.8531, -0.2261,  0.4007,  0.1298, -0.5879, -0.3480, -0.5656,  0.1757,\n",
      "          0.0059,  0.2265, -0.3781,  0.3005,  0.6822,  0.6380, -0.0063, -0.2918,\n",
      "          0.5183,  0.8661, -0.8274,  0.0274, -0.2601, -0.2313, -0.4456,  0.1319,\n",
      "         -1.1137, -0.3110,  0.0656,  0.4440,  0.3891, -0.6426,  0.6306,  0.1538,\n",
      "         -0.4204,  0.8614, -0.0014, -0.1679,  0.9535, -0.6094, -0.4573, -0.0518,\n",
      "         -0.2752,  0.2438, -0.2697,  0.0497,  0.0221,  0.5697,  0.9108, -0.3637,\n",
      "         -0.2279, -0.6924, -0.0224,  0.7056, -0.1947, -0.5959, -0.3067, -0.0132]]), tensor([[ 0.3846, -0.1098,  0.3073,  0.6073, -0.1923, -0.7427,  0.2459,  0.6868,\n",
      "         -0.7579, -0.0845,  0.3094,  0.1413, -0.2822, -0.2871, -0.3603,  0.0904,\n",
      "         -0.1708,  0.1598, -0.3850,  0.3047,  0.5782,  0.6798,  0.1213, -0.1310,\n",
      "          0.3151,  0.6727, -0.8165, -0.1731, -0.2908, -0.0727, -0.5041,  0.2139,\n",
      "         -0.8426, -0.4303,  0.0984,  0.3529,  0.3550, -0.2864,  0.6397,  0.1055,\n",
      "         -0.0761,  0.7222,  0.0290, -0.3308,  0.7007, -0.5372, -0.4417, -0.0551,\n",
      "         -0.2125,  0.3457, -0.0877,  0.1968,  0.0332,  0.4942,  0.7871, -0.3206,\n",
      "         -0.1354, -0.4670,  0.0591,  0.4187, -0.3089, -0.5153, -0.5297, -0.1338]]), tensor([[ 0.3353, -0.2957,  0.6356,  0.5595, -0.0573, -0.9055,  0.4860,  0.3462,\n",
      "         -0.5609, -0.1639,  0.3792,  0.3434, -0.1598, -0.1728, -0.1863, -0.1074,\n",
      "          0.0740,  0.3559, -0.5989,  0.3159,  0.2770,  0.5868, -0.2406,  0.0272,\n",
      "          0.5691,  0.8045, -0.9196,  0.0999, -0.3229, -0.4021, -0.5581, -0.1165,\n",
      "         -1.1617, -0.2002,  0.0763,  0.4488,  0.3345, -0.5555,  0.8507,  0.2137,\n",
      "         -0.3695,  1.0635,  0.0647, -0.1067,  0.9021, -0.6945, -0.4263, -0.1736,\n",
      "         -0.2612, -0.0664, -0.1634, -0.0384,  0.2396,  0.5976,  0.8230, -0.3018,\n",
      "         -0.4901, -0.7526, -0.3003,  0.6341, -0.6943, -0.6284, -0.8384, -0.1524]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n",
      "tensor([3])\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_DSet, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_DSet, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256c456",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a981bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (ff): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=48, bias=False)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (rnn): RNN(48, 16, bias=False)\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=6, bias=False)\n",
      "    (1): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" Builds recurrent neural network model \"\"\"\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(64, 48, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.rnn = nn.RNN(48, 16, nonlinearity='relu', bias=False)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(16, 6, bias=False),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, q):\n",
    "        \"\"\" Implements feed-forward then recurrent layer \"\"\"\n",
    "        ff_q = []\n",
    "        for word in q:\n",
    "            ff_q.append(self.ff(word))\n",
    "        ff_q = torch.stack(ff_q)\n",
    "        h_N = Variable(torch.zeros(1, 16)).to(device)\n",
    "        for word in ff_q:\n",
    "            if not torch.all(word.eq(0)):\n",
    "                rnn_out, h_N = self.rnn(word, h_N)\n",
    "            else:\n",
    "                break\n",
    "        output = self.out(rnn_out)\n",
    "        return output\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf8660e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mqint4\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.qint4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a554826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model\n",
    "\n",
    "param_list = [*model.parameters()]\n",
    "len(param_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f51ab5",
   "metadata": {},
   "source": [
    "## Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873219a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss function and optimizer\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# define training routine\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = torch.stack(X).to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error and optimize\n",
    "        optimizer.zero_grad()\n",
    "        for word in X:\n",
    "            if torch.all(word.eq(0)):\n",
    "                print(\"FLAG!\")\n",
    "                print(X)\n",
    "                break\n",
    "            else:\n",
    "                break\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # clamp diagonal hidden-hidden weights of RNN layer to 0\n",
    "        model.rnn._parameters['weight_hh_l0'].data.diagonal().clamp_(min=0, max=0)\n",
    "        \n",
    "        # print loss and accuracy at selected iterations\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# define test routine\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = torch.stack(X).to(device), y.to(device) #torch.FloatTensor(y).to(device)\n",
    "            pred = model(X)\n",
    "            target = y#.argmax(1)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f2121fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3ddd2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.115939  [    1/ 5000]\n",
      "loss: 0.000001  [ 1001/ 5000]\n",
      "loss: 1.166997  [ 2001/ 5000]\n",
      "loss: 0.673601  [ 3001/ 5000]\n",
      "loss: 1.069983  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.796557 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.629578  [    1/ 5000]\n",
      "loss: 1.545170  [ 1001/ 5000]\n",
      "loss: 0.558245  [ 2001/ 5000]\n",
      "loss: 0.004281  [ 3001/ 5000]\n",
      "loss: 0.016951  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.787113 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.344981  [    1/ 5000]\n",
      "loss: 0.496069  [ 1001/ 5000]\n",
      "loss: 0.483845  [ 2001/ 5000]\n",
      "loss: 0.234901  [ 3001/ 5000]\n",
      "loss: 0.318510  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.767776 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.039172  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.135399  [ 2001/ 5000]\n",
      "loss: 0.182714  [ 3001/ 5000]\n",
      "loss: 0.446136  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.726187 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.219875  [    1/ 5000]\n",
      "loss: 0.760492  [ 1001/ 5000]\n",
      "loss: 1.251870  [ 2001/ 5000]\n",
      "loss: 1.733736  [ 3001/ 5000]\n",
      "loss: 0.008749  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.799702 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.225765  [    1/ 5000]\n",
      "loss: 0.002607  [ 1001/ 5000]\n",
      "loss: 1.905226  [ 2001/ 5000]\n",
      "loss: 0.013114  [ 3001/ 5000]\n",
      "loss: 2.349902  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.753451 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.659448  [    1/ 5000]\n",
      "loss: 1.831485  [ 1001/ 5000]\n",
      "loss: 1.119974  [ 2001/ 5000]\n",
      "loss: 1.990470  [ 3001/ 5000]\n",
      "loss: 0.008097  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.791413 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.370038  [    1/ 5000]\n",
      "loss: 0.624073  [ 1001/ 5000]\n",
      "loss: 2.846080  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 0.000174  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.751716 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.000216  [    1/ 5000]\n",
      "loss: 0.124430  [ 1001/ 5000]\n",
      "loss: 0.212550  [ 2001/ 5000]\n",
      "loss: 0.000013  [ 3001/ 5000]\n",
      "loss: 0.014865  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.787615 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.005535  [    1/ 5000]\n",
      "loss: 2.113221  [ 1001/ 5000]\n",
      "loss: 0.129928  [ 2001/ 5000]\n",
      "loss: 0.027989  [ 3001/ 5000]\n",
      "loss: 0.058259  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.767046 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.042448  [    1/ 5000]\n",
      "loss: 0.010200  [ 1001/ 5000]\n",
      "loss: 0.008862  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 8.296751  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.829504 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.036380  [    1/ 5000]\n",
      "loss: 8.584410  [ 1001/ 5000]\n",
      "loss: 0.000340  [ 2001/ 5000]\n",
      "loss: 0.482596  [ 3001/ 5000]\n",
      "loss: 1.511032  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.772306 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.032934  [    1/ 5000]\n",
      "loss: 1.475310  [ 1001/ 5000]\n",
      "loss: 0.381025  [ 2001/ 5000]\n",
      "loss: 0.798077  [ 3001/ 5000]\n",
      "loss: 0.766199  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.758226 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.000407  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.141310  [ 2001/ 5000]\n",
      "loss: 0.156534  [ 3001/ 5000]\n",
      "loss: 1.435067  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.858746 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.038788  [    1/ 5000]\n",
      "loss: 2.586955  [ 1001/ 5000]\n",
      "loss: 3.222450  [ 2001/ 5000]\n",
      "loss: 1.085140  [ 3001/ 5000]\n",
      "loss: 0.570124  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.800653 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.049029  [    1/ 5000]\n",
      "loss: 0.000007  [ 1001/ 5000]\n",
      "loss: 0.007576  [ 2001/ 5000]\n",
      "loss: 1.685969  [ 3001/ 5000]\n",
      "loss: 1.047759  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.738169 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.268334  [    1/ 5000]\n",
      "loss: 0.002800  [ 1001/ 5000]\n",
      "loss: 0.037713  [ 2001/ 5000]\n",
      "loss: 0.833524  [ 3001/ 5000]\n",
      "loss: 3.329259  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.809273 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 3.154932  [    1/ 5000]\n",
      "loss: 0.005654  [ 1001/ 5000]\n",
      "loss: 5.640099  [ 2001/ 5000]\n",
      "loss: 2.205483  [ 3001/ 5000]\n",
      "loss: 2.721678  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.741976 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.000127  [    1/ 5000]\n",
      "loss: 0.024621  [ 1001/ 5000]\n",
      "loss: 0.975909  [ 2001/ 5000]\n",
      "loss: 0.011610  [ 3001/ 5000]\n",
      "loss: 0.000000  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.837870 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.016367  [    1/ 5000]\n",
      "loss: 1.368209  [ 1001/ 5000]\n",
      "loss: 0.000402  [ 2001/ 5000]\n",
      "loss: 0.533955  [ 3001/ 5000]\n",
      "loss: 0.000179  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.791136 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.305939  [    1/ 5000]\n",
      "loss: 0.518600  [ 1001/ 5000]\n",
      "loss: 0.089017  [ 2001/ 5000]\n",
      "loss: 0.105715  [ 3001/ 5000]\n",
      "loss: 3.233807  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.757179 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.799317  [    1/ 5000]\n",
      "loss: 0.028456  [ 1001/ 5000]\n",
      "loss: 0.558143  [ 2001/ 5000]\n",
      "loss: 0.000044  [ 3001/ 5000]\n",
      "loss: 2.914583  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.852379 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.139183  [    1/ 5000]\n",
      "loss: 0.679497  [ 1001/ 5000]\n",
      "loss: 0.000707  [ 2001/ 5000]\n",
      "loss: 1.520308  [ 3001/ 5000]\n",
      "loss: 0.106164  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.866774 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.087379  [    1/ 5000]\n",
      "loss: 0.002845  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.828275  [ 3001/ 5000]\n",
      "loss: 0.057848  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.900750 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.030543  [    1/ 5000]\n",
      "loss: 0.068386  [ 1001/ 5000]\n",
      "loss: 0.206465  [ 2001/ 5000]\n",
      "loss: 0.000821  [ 3001/ 5000]\n",
      "loss: 0.025819  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.842383 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 3.510998  [    1/ 5000]\n",
      "loss: 0.066669  [ 1001/ 5000]\n",
      "loss: 0.007418  [ 2001/ 5000]\n",
      "loss: 0.202190  [ 3001/ 5000]\n",
      "loss: 0.000006  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.859900 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.001285  [    1/ 5000]\n",
      "loss: 0.371828  [ 1001/ 5000]\n",
      "loss: 2.808257  [ 2001/ 5000]\n",
      "loss: 0.022220  [ 3001/ 5000]\n",
      "loss: 0.093820  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.831082 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.467723  [    1/ 5000]\n",
      "loss: 1.036226  [ 1001/ 5000]\n",
      "loss: 0.556438  [ 2001/ 5000]\n",
      "loss: 0.870391  [ 3001/ 5000]\n",
      "loss: 0.758421  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.799077 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.473503  [    1/ 5000]\n",
      "loss: 1.446214  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.098095  [ 3001/ 5000]\n",
      "loss: 0.154746  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.832768 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.059624  [    1/ 5000]\n",
      "loss: 0.404577  [ 1001/ 5000]\n",
      "loss: 0.274289  [ 2001/ 5000]\n",
      "loss: 0.002273  [ 3001/ 5000]\n",
      "loss: 0.084930  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.789035 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.007818  [    1/ 5000]\n",
      "loss: 0.000001  [ 1001/ 5000]\n",
      "loss: 0.406983  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 1.357120  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.849290 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.918529  [    1/ 5000]\n",
      "loss: 0.004526  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.819349  [ 3001/ 5000]\n",
      "loss: 0.003995  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.811577 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.013332  [    1/ 5000]\n",
      "loss: 0.504093  [ 1001/ 5000]\n",
      "loss: 0.468009  [ 2001/ 5000]\n",
      "loss: 0.000040  [ 3001/ 5000]\n",
      "loss: 0.000009  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.788933 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.689327  [    1/ 5000]\n",
      "loss: 0.035676  [ 1001/ 5000]\n",
      "loss: 0.442058  [ 2001/ 5000]\n",
      "loss: 1.184602  [ 3001/ 5000]\n",
      "loss: 0.002608  [ 4001/ 5000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.888721 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.037196  [    1/ 5000]\n",
      "loss: 0.000156  [ 1001/ 5000]\n",
      "loss: 0.012394  [ 2001/ 5000]\n",
      "loss: 0.000389  [ 3001/ 5000]\n",
      "loss: 0.000002  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.903190 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.007299  [    1/ 5000]\n",
      "loss: 0.000213  [ 1001/ 5000]\n",
      "loss: 0.562013  [ 2001/ 5000]\n",
      "loss: 1.536533  [ 3001/ 5000]\n",
      "loss: 0.000000  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.821677 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 8.949102  [    1/ 5000]\n",
      "loss: 0.142331  [ 1001/ 5000]\n",
      "loss: 2.277160  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 0.006997  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.917136 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.337495  [    1/ 5000]\n",
      "loss: 0.082520  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.035197  [ 3001/ 5000]\n",
      "loss: 1.696650  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.846966 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 6.421303  [    1/ 5000]\n",
      "loss: 0.371249  [ 1001/ 5000]\n",
      "loss: 0.044556  [ 2001/ 5000]\n",
      "loss: 1.374296  [ 3001/ 5000]\n",
      "loss: 0.024943  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.862827 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.000567  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.002307  [ 2001/ 5000]\n",
      "loss: 0.539243  [ 3001/ 5000]\n",
      "loss: 1.021606  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.832007 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.033394  [    1/ 5000]\n",
      "loss: 0.000298  [ 1001/ 5000]\n",
      "loss: 0.091276  [ 2001/ 5000]\n",
      "loss: 0.003813  [ 3001/ 5000]\n",
      "loss: 0.876777  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.778689 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.008596  [    1/ 5000]\n",
      "loss: 0.000211  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 1.472366  [ 3001/ 5000]\n",
      "loss: 1.031653  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.918742 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.917007  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.003160  [ 2001/ 5000]\n",
      "loss: 0.098778  [ 3001/ 5000]\n",
      "loss: 0.041406  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.822906 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.012626  [    1/ 5000]\n",
      "loss: 2.221927  [ 1001/ 5000]\n",
      "loss: 0.237331  [ 2001/ 5000]\n",
      "loss: 0.000775  [ 3001/ 5000]\n",
      "loss: 0.010399  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.870188 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.093329  [    1/ 5000]\n",
      "loss: 1.090384  [ 1001/ 5000]\n",
      "loss: 0.000948  [ 2001/ 5000]\n",
      "loss: 0.002131  [ 3001/ 5000]\n",
      "loss: 0.074068  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.885433 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.017707  [    1/ 5000]\n",
      "loss: 0.242909  [ 1001/ 5000]\n",
      "loss: 0.017626  [ 2001/ 5000]\n",
      "loss: 1.192057  [ 3001/ 5000]\n",
      "loss: 0.262553  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.947838 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.130522  [    1/ 5000]\n",
      "loss: 0.040257  [ 1001/ 5000]\n",
      "loss: 0.540218  [ 2001/ 5000]\n",
      "loss: 0.589464  [ 3001/ 5000]\n",
      "loss: 0.180647  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.875766 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.000865  [    1/ 5000]\n",
      "loss: 0.840403  [ 1001/ 5000]\n",
      "loss: 0.000110  [ 2001/ 5000]\n",
      "loss: 0.246726  [ 3001/ 5000]\n",
      "loss: 0.002213  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 1.044776 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.043151  [    1/ 5000]\n",
      "loss: 0.088567  [ 1001/ 5000]\n",
      "loss: 6.550188  [ 2001/ 5000]\n",
      "loss: 1.562441  [ 3001/ 5000]\n",
      "loss: 0.000013  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.896763 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.000000  [    1/ 5000]\n",
      "loss: 0.000007  [ 1001/ 5000]\n",
      "loss: 0.074869  [ 2001/ 5000]\n",
      "loss: 0.031038  [ 3001/ 5000]\n",
      "loss: 0.208096  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.793046 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.397650  [    1/ 5000]\n",
      "loss: 1.721866  [ 1001/ 5000]\n",
      "loss: 0.938010  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 0.037131  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.819124 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 1.058255  [    1/ 5000]\n",
      "loss: 0.003117  [ 1001/ 5000]\n",
      "loss: 0.341106  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 0.000001  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.902897 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.893490  [    1/ 5000]\n",
      "loss: 0.000482  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.345867  [ 3001/ 5000]\n",
      "loss: 0.571011  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.788819 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 1.276160  [    1/ 5000]\n",
      "loss: 0.033953  [ 1001/ 5000]\n",
      "loss: 0.096307  [ 2001/ 5000]\n",
      "loss: 1.219272  [ 3001/ 5000]\n",
      "loss: 0.000000  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.863986 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.001070  [    1/ 5000]\n",
      "loss: 0.436959  [ 1001/ 5000]\n",
      "loss: 0.000001  [ 2001/ 5000]\n",
      "loss: 1.279820  [ 3001/ 5000]\n",
      "loss: 0.005694  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.883776 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.000180  [    1/ 5000]\n",
      "loss: 0.890173  [ 1001/ 5000]\n",
      "loss: 1.288750  [ 2001/ 5000]\n",
      "loss: 0.734408  [ 3001/ 5000]\n",
      "loss: 0.023246  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.942076 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.000009  [    1/ 5000]\n",
      "loss: 0.091591  [ 1001/ 5000]\n",
      "loss: 0.434164  [ 2001/ 5000]\n",
      "loss: 0.001348  [ 3001/ 5000]\n",
      "loss: 0.111035  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.931843 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.941766  [    1/ 5000]\n",
      "loss: 0.416717  [ 1001/ 5000]\n",
      "loss: 0.000007  [ 2001/ 5000]\n",
      "loss: 1.905327  [ 3001/ 5000]\n",
      "loss: 3.480450  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.932447 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.009396  [    1/ 5000]\n",
      "loss: 1.041537  [ 1001/ 5000]\n",
      "loss: 0.811538  [ 2001/ 5000]\n",
      "loss: 0.490598  [ 3001/ 5000]\n",
      "loss: 0.005317  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.857966 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.353175  [    1/ 5000]\n",
      "loss: 0.849225  [ 1001/ 5000]\n",
      "loss: 0.327566  [ 2001/ 5000]\n",
      "loss: 0.000016  [ 3001/ 5000]\n",
      "loss: 0.438869  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.828692 \n",
      "\n",
      "Done!\n",
      "\n",
      "Total epochs: 100\n"
     ]
    }
   ],
   "source": [
    "# perform training and test performance over epochs\n",
    "\n",
    "epochs = 60\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "total_epochs += epochs\n",
    "print(\"\\nTotal epochs: {0}\".format(total_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfc45f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case training takes a wrong turn\n",
    "temp_model_backup = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6b18c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "torch.Size([48, 64])\n",
      "Parameter containing:\n",
      "tensor([[-0.0706,  0.8817,  0.1850,  ..., -0.0875,  0.0828,  0.6999],\n",
      "        [ 0.9073,  0.2710,  0.9453,  ..., -0.1008, -0.8766, -1.6422],\n",
      "        [ 0.1462,  0.1366,  0.5575,  ..., -0.7116, -0.1804, -0.7887],\n",
      "        ...,\n",
      "        [-0.7479, -0.2158,  0.9840,  ...,  0.0507,  0.1247,  0.4779],\n",
      "        [-0.0898, -0.0664,  0.1271,  ..., -0.3905,  0.2266,  0.6929],\n",
      "        [-0.1669, -0.0281, -0.6965,  ...,  0.7204,  1.1612, -0.3478]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Layer 2\n",
      "torch.Size([16, 48])\n",
      "Parameter containing:\n",
      "tensor([[-6.4465e-02, -1.0213e+00,  9.9517e-01,  4.1901e-01, -3.5492e-01,\n",
      "         -1.1138e+00, -4.2743e-01,  5.3006e-01, -1.8814e+00,  3.3486e-01,\n",
      "          4.3346e-01,  5.5507e-01,  2.1036e-01, -1.5811e-01, -1.6214e-01,\n",
      "          4.8606e-01, -8.9539e-01, -1.2283e+00, -3.0397e-01, -1.6822e+00,\n",
      "         -2.2543e+00, -1.4987e+00, -1.9769e+00,  1.3467e+00,  9.5415e-02,\n",
      "         -1.3498e-01, -3.2551e+00,  3.0033e+00, -8.6687e-01,  4.9060e-01,\n",
      "         -1.4485e-01,  1.1442e-03,  1.0198e+00,  1.5178e+00,  6.1239e-01,\n",
      "         -2.9515e-01, -5.0114e-01, -1.2140e+00,  6.3014e-01,  1.3494e-01,\n",
      "          1.9487e+00, -3.2424e+00, -4.2376e-01,  4.0906e-01, -9.1445e-01,\n",
      "         -1.0616e+00,  2.9721e+00,  3.4222e-01],\n",
      "        [-9.6724e-01, -1.8939e+00,  6.4935e-01,  5.3268e-01, -4.9286e-02,\n",
      "         -5.9577e-01, -2.7099e-01,  2.9752e-01,  1.4967e-01, -1.4439e+00,\n",
      "         -1.0790e+00, -1.5271e+00,  4.9943e-01,  9.2210e-01, -2.7338e-01,\n",
      "         -1.4530e-01,  3.8921e-01,  1.1706e+00, -1.4377e-01,  3.0801e-01,\n",
      "          7.6275e-01, -3.6534e-01,  1.2519e-01,  3.8159e-01, -9.4465e-01,\n",
      "         -9.5433e-01,  2.0975e+00,  7.4434e-02, -2.7007e-02,  1.3357e+00,\n",
      "          1.0033e+00, -5.9273e-01,  8.5202e-02,  1.6500e-01,  8.2523e-01,\n",
      "          3.5281e-01, -7.3520e-01, -1.4201e+00, -1.4070e-01, -1.1727e+00,\n",
      "          6.6426e-01,  1.4326e+00,  1.4410e+00, -5.7961e-01, -3.6872e-02,\n",
      "         -1.6557e-01, -6.2917e-01, -2.4643e-01],\n",
      "        [-3.6291e-02, -9.6026e-01,  4.4036e-01, -8.4765e-01, -4.1282e-01,\n",
      "         -1.3389e+00,  1.1203e+00, -5.8589e-01, -2.5715e+00,  5.3739e-02,\n",
      "         -1.6609e+00, -4.6983e-01, -1.0053e+00, -2.4954e-01,  5.5552e-01,\n",
      "          1.1384e+00, -5.4140e-02, -8.4217e-01, -4.0906e-02,  5.2526e-01,\n",
      "          5.7468e-01,  2.1382e-01, -2.3010e-01, -1.0347e+00, -7.0896e-02,\n",
      "          5.2876e-01, -1.3521e+00, -3.2839e-01,  1.4128e-02,  4.6281e-01,\n",
      "          4.9300e-01, -1.5115e+00,  1.2427e-01, -2.8171e-01,  1.5065e-01,\n",
      "          9.6629e-01, -6.0456e-01, -1.3952e+00,  4.5253e-01, -2.5637e-01,\n",
      "          8.1791e-01,  4.5287e-01,  3.3074e-01, -3.4928e-01, -1.4385e-01,\n",
      "         -2.8036e-01, -7.6702e-01, -1.3080e+00],\n",
      "        [ 2.7552e-01,  5.1512e-01, -1.0002e-01, -3.8172e-01,  9.1034e-01,\n",
      "         -7.0121e-02, -1.8358e+00,  1.4905e+00,  1.9073e+00, -2.4335e-01,\n",
      "         -1.1066e+00,  3.8699e-02, -8.8807e-01,  2.6393e-01,  1.1245e+00,\n",
      "          4.5231e-01, -7.2488e-01, -2.3932e-01, -1.4916e+00,  9.0752e-01,\n",
      "         -4.7449e-01, -1.6158e-01, -3.3632e-02,  1.0281e-01,  1.1360e+00,\n",
      "         -5.7771e-01,  2.5811e+00, -2.9322e-01, -1.1222e-01, -5.6941e-01,\n",
      "          4.2561e-01, -4.2022e-01,  1.6557e-01, -7.5260e-01,  1.0519e-01,\n",
      "          5.5796e-01,  1.8792e+00, -5.7433e-01,  1.0426e-01,  1.3753e-01,\n",
      "         -2.5266e-01,  2.1174e+00, -1.7858e-02, -6.7607e-01,  6.6988e-01,\n",
      "         -2.0682e-01, -4.4304e-01, -2.9909e-01],\n",
      "        [ 1.6477e-01, -3.2040e-01, -1.8254e-01,  6.9820e-01,  5.9514e-01,\n",
      "         -8.4973e-01,  1.1154e+00, -1.1461e-02,  8.8198e-01, -1.7020e-01,\n",
      "         -5.4394e-01,  4.9153e-01,  2.5492e-01, -3.1082e-01, -3.6106e-01,\n",
      "         -3.1418e-01,  4.5488e-01,  1.8206e+00, -5.1511e-01,  1.2763e-01,\n",
      "          4.9997e-01,  3.4223e-01, -1.0308e+00, -2.5202e+00, -7.4089e-02,\n",
      "         -6.8489e-01,  6.3066e+00,  3.2833e+00,  1.7241e-01,  2.2347e-01,\n",
      "         -1.4112e-01,  5.9258e-01,  1.5938e-01, -1.3972e+00,  1.0049e+00,\n",
      "          9.5912e-01,  9.1487e-01,  1.1424e+00,  2.1026e-01,  4.0968e-01,\n",
      "         -3.0299e-01,  8.4264e-01, -2.6958e-01,  2.8906e-01,  2.5753e-01,\n",
      "          1.6497e-01, -1.3786e+00,  2.4549e-01],\n",
      "        [ 6.1119e-01, -1.0987e+00, -2.0262e-01,  4.4934e-01, -7.3158e-02,\n",
      "          2.8306e-01, -6.5666e-01,  5.8283e-01, -6.6656e-02, -2.5195e-01,\n",
      "         -2.9483e-01,  9.2283e-01, -2.2316e-01,  2.8609e-01,  3.1816e-01,\n",
      "          7.7307e-01, -4.2671e-02,  1.5255e+00,  1.1165e-01,  4.4258e-01,\n",
      "         -2.0729e-01,  2.8080e-02,  4.4755e-02, -3.0034e-01,  6.7649e-01,\n",
      "          4.2507e-01, -1.0895e+00,  5.4434e-01,  6.5270e-01,  1.0181e-01,\n",
      "          1.6644e-01, -1.9828e-01,  6.5798e-01,  9.3836e-01, -7.0125e-01,\n",
      "          5.6738e-01,  8.1024e-01,  1.7877e+00,  3.5505e-02,  6.5579e-01,\n",
      "         -1.7546e-01,  3.2677e-02,  7.0950e-01,  2.9482e-02,  3.5909e-01,\n",
      "         -1.4264e-01,  1.3296e+00,  3.2598e-01],\n",
      "        [-4.2216e-01,  8.8755e-01,  5.3209e-01, -6.3365e-01, -1.2046e+00,\n",
      "          3.9121e-01,  7.3000e+00,  8.7009e-01, -8.4827e-02,  2.1403e-01,\n",
      "          2.3185e-01,  2.6598e+00,  2.3698e-01,  3.3822e-01,  8.2269e-01,\n",
      "         -7.6041e-01, -3.3041e-01, -1.8790e-01,  1.4054e-01,  1.4557e-01,\n",
      "         -5.7410e-01, -1.0613e-01, -2.2973e-01,  1.6395e+00, -1.1968e+00,\n",
      "          2.7175e-01, -3.5996e+00,  2.6500e-01,  1.8065e-01,  9.2117e-01,\n",
      "          2.3626e-01, -1.9663e-02,  7.5652e-01,  4.8230e-01, -1.7452e+00,\n",
      "         -3.6510e-01,  1.2369e+00,  1.4025e+00,  5.9677e-01, -1.1806e-01,\n",
      "          8.5710e-02,  4.2873e-02, -9.6839e-02, -1.0466e+00, -9.3150e-01,\n",
      "          9.9300e-02,  8.2705e-01, -3.1942e-01],\n",
      "        [-7.6918e-01, -4.4414e-01,  1.7553e-01, -2.0268e+00, -8.7563e-01,\n",
      "          1.2450e+00, -1.1678e-01, -9.5076e-02, -1.2452e+00, -3.6510e-01,\n",
      "          4.7402e-01,  4.0721e-01,  1.9267e+00, -6.1328e-01, -1.5688e-01,\n",
      "         -7.3289e-01,  2.5432e-01, -1.2893e+00,  4.4484e-02, -5.1597e-01,\n",
      "         -1.1538e-01, -6.9500e-01, -5.5139e-01, -1.8889e+00,  1.8727e+00,\n",
      "         -1.0146e+00, -1.9708e-01, -1.4037e+00, -2.3150e+00,  7.1286e-01,\n",
      "          2.7244e-01,  1.4560e+00,  6.3296e-01, -4.8870e-02, -1.9378e-01,\n",
      "         -1.1332e+00, -1.3947e+00, -6.9404e-01, -6.0318e-01, -2.0724e-01,\n",
      "          8.5036e-01,  9.4704e-01, -1.6530e-01, -2.0909e+00, -3.5328e-01,\n",
      "         -2.7759e-01,  1.1468e+00,  4.9572e-01],\n",
      "        [ 3.1928e-01, -5.8999e-01,  3.4012e-01, -9.7617e-01, -8.2612e-01,\n",
      "         -7.4891e-01, -2.1671e+00, -1.1863e+00, -1.1288e-01, -2.5979e+00,\n",
      "          2.5891e-01, -1.6241e+00,  9.0185e-01,  4.2996e-01, -5.2291e-01,\n",
      "          5.7844e-01, -3.1702e-01,  1.6266e-01,  3.6081e-01, -4.9023e-01,\n",
      "         -4.0575e-02, -4.6130e-01,  5.0250e-01,  4.4661e-01,  2.9446e-01,\n",
      "         -7.5142e-01,  2.8969e+00, -1.1637e+00,  7.5838e-01,  2.2094e-01,\n",
      "          1.0308e+00,  3.2410e-01, -4.7291e-01, -1.3914e+00,  1.3293e+00,\n",
      "         -5.5340e-01, -2.7607e-01, -1.2133e+00,  4.9020e-01,  3.2234e-01,\n",
      "          1.0043e+00, -1.7533e+00,  1.7104e-01, -2.3431e-01, -7.0812e-01,\n",
      "         -1.0206e-01, -3.4692e+00,  5.2730e-01],\n",
      "        [ 4.4038e-01, -5.0692e-01,  5.0121e-01, -7.8852e-01, -4.9278e-01,\n",
      "         -6.1785e-01, -9.8035e-01,  1.4256e-01,  4.9312e-01, -9.3613e-02,\n",
      "          3.8615e-01, -3.4996e-01, -5.3025e-01, -2.3461e-01,  2.2816e-02,\n",
      "          9.1859e-01,  8.0857e-01, -5.9616e-02, -4.5246e-01, -4.4358e-01,\n",
      "         -7.9142e-01,  2.4576e-02, -3.7825e-01,  1.0848e+00,  3.9552e-01,\n",
      "          9.2960e-01,  1.5277e+00,  9.9757e-01, -2.5006e-02,  1.0543e+00,\n",
      "          9.5464e-02, -3.1671e-01,  7.3629e-01,  8.0619e-02, -1.5664e-01,\n",
      "         -2.9432e-01,  3.9663e-02, -6.0169e-01, -1.8301e-01,  1.0641e-01,\n",
      "         -8.4640e-03, -3.0173e-02, -6.0263e-01,  2.6321e-01,  7.4279e-01,\n",
      "          1.5586e-01,  4.3428e-01,  6.2301e-01],\n",
      "        [ 1.2317e-01, -1.9980e-01,  1.0540e-01,  5.5547e-02,  4.4577e-02,\n",
      "         -2.6927e-01,  5.2490e+00, -4.8845e-01, -5.8211e-02, -9.3738e-02,\n",
      "         -1.9012e-01,  5.5131e-01,  3.0661e-01,  8.1458e-01, -6.7736e-01,\n",
      "         -1.0750e-01, -4.1115e-01,  3.1691e-02, -5.4536e-01, -1.4209e-01,\n",
      "         -3.4900e-01,  9.3023e-01, -1.2796e-01,  7.2784e-01,  2.4103e-01,\n",
      "         -3.0714e-01, -1.1545e-01, -8.2666e-01,  8.0164e-01,  3.1473e-02,\n",
      "          1.2410e-01, -3.7728e-01,  1.2580e-02,  1.8733e+00,  5.9253e-01,\n",
      "          2.7081e-01, -3.3144e-01,  1.4302e+00,  5.2522e-01,  3.8179e-01,\n",
      "         -2.6773e-01,  2.3242e-01, -1.5143e-02,  3.4540e-01, -8.0051e-01,\n",
      "          7.9813e-02, -1.0914e+00,  1.4997e-01],\n",
      "        [ 3.1824e-01,  1.5398e+00,  4.0615e-02,  7.1879e-01,  6.9648e-01,\n",
      "         -6.9649e-01, -2.1587e+00,  9.3865e-01, -2.1625e-01,  3.6662e-02,\n",
      "         -6.8115e-01,  2.0710e+00,  7.3287e-01,  1.1107e-01,  2.8488e-01,\n",
      "          1.1137e+00, -1.1841e+00,  7.3804e-01, -1.0022e+00,  1.2404e+00,\n",
      "          3.7723e-01, -1.2107e+00, -1.0536e-01,  1.6094e+00,  1.8446e-01,\n",
      "          5.7423e-01,  2.6007e+00,  1.0129e+00, -1.2407e-01,  2.9412e-01,\n",
      "          1.4355e-01, -1.0625e+00,  2.4029e-01,  4.1150e-01, -1.8848e-01,\n",
      "          5.8587e-01, -5.6734e-01,  2.3937e+00, -3.4802e-01, -2.1478e-02,\n",
      "         -2.8042e-01, -2.4445e-01, -6.8270e-01, -4.2400e-02,  5.3844e-01,\n",
      "         -4.1845e-01,  5.0005e-01,  8.2551e-02],\n",
      "        [ 6.3719e-01, -4.2884e-01,  4.8704e-01, -1.8014e-01, -6.3230e-01,\n",
      "          4.0399e-01,  1.6846e+00,  1.4057e-01, -3.1977e-01,  2.0235e-01,\n",
      "         -1.5041e-01,  1.8987e+00,  1.1376e+00, -1.0917e+00,  5.9625e-01,\n",
      "         -1.9734e-01,  4.2873e-01, -8.4307e-02, -3.4830e-01, -3.7132e-01,\n",
      "         -7.2328e-01,  3.7013e-01, -7.2717e-02, -1.5166e-01,  2.1931e-01,\n",
      "          1.4026e-01, -1.6396e+00,  2.7429e+00, -7.0047e-01,  6.6170e-01,\n",
      "          1.5077e-01, -2.2569e+00, -8.8708e-01,  1.1259e+00, -7.2906e-01,\n",
      "          1.4160e-01,  2.1308e-01,  2.5435e+00,  2.9759e-01, -4.5443e-01,\n",
      "          1.2053e+00, -2.6023e-01, -6.2627e-01,  1.9672e-01,  1.5073e-01,\n",
      "          6.7088e-01,  1.4693e+00, -5.7104e-02],\n",
      "        [-1.9424e-01,  3.1358e-01,  1.4306e-01, -4.2888e-01,  3.3348e-01,\n",
      "         -9.2991e-01, -1.2454e+00,  4.4460e-01,  5.9086e-01, -2.5489e-01,\n",
      "          1.2582e-01, -3.8618e+00,  1.2593e+00, -4.9214e-01, -1.4298e-02,\n",
      "          3.5313e-01, -2.1818e-01, -4.2897e-01, -6.0591e-01,  9.5959e-02,\n",
      "          1.0931e+00, -2.3327e-01,  3.1157e-01,  5.8499e-01, -1.0867e-01,\n",
      "         -4.5682e-01,  3.3578e+00, -2.5553e+00, -2.3824e-01, -1.0224e+00,\n",
      "         -1.2224e+00, -6.5810e-01,  7.1884e-02, -6.0126e-01,  1.1561e+00,\n",
      "          3.1596e-01,  1.0625e+00, -2.2625e-01,  1.3284e-01,  4.0459e-01,\n",
      "         -5.2784e-01,  3.3229e-03, -2.2338e-01, -1.6667e-01, -1.0149e+00,\n",
      "         -4.1630e-01, -2.0736e+00,  4.4872e-01],\n",
      "        [ 5.3964e-01,  6.6746e-02,  4.5205e-01,  6.3987e-01, -2.9823e-01,\n",
      "         -9.7066e-01, -4.0419e-01, -2.1844e-02, -9.8613e-01, -2.2444e-01,\n",
      "         -1.0621e+00, -1.3146e+00,  5.8347e-01, -2.9098e-03, -5.4731e-01,\n",
      "          1.3457e+00, -9.5634e-02,  1.1064e+00,  2.3559e-01,  3.9940e-02,\n",
      "         -5.9881e-01, -1.3582e-01,  1.1940e-01,  4.9434e-01,  7.0584e-02,\n",
      "          4.9657e-01,  4.2158e+00, -7.9972e-01,  1.3707e-01, -7.8293e-01,\n",
      "          8.0374e-01, -1.5926e-01, -4.3550e-01, -2.8026e-02,  2.6776e-01,\n",
      "          2.5190e-01, -7.9239e-01, -5.6659e-01,  1.2307e-01, -3.6408e-01,\n",
      "         -1.1884e-01,  5.1020e-02,  4.0678e-01, -3.3068e-01,  9.0990e-02,\n",
      "         -1.0464e+00, -1.8344e-01, -3.1419e-01],\n",
      "        [-2.5342e+00,  3.3528e-01,  2.0385e-01,  5.0025e-02, -7.7504e-01,\n",
      "          1.0101e+00,  7.7707e-02,  1.2461e+00, -4.1496e-01, -1.3250e-01,\n",
      "         -2.2185e-02, -2.2654e+00,  4.3126e-02,  7.0925e-01,  1.1476e+00,\n",
      "         -6.6559e-01, -3.9683e-01,  8.7787e-01, -3.7173e-01, -4.3547e-01,\n",
      "         -1.1075e-01,  3.5470e-02,  2.7125e-01,  3.8857e-02, -7.9070e-01,\n",
      "          3.6796e-01,  2.5625e+00, -1.6712e-01, -2.0179e-01,  6.4021e-01,\n",
      "          1.9596e-01, -8.3231e-02, -3.3570e-01,  4.2477e-01,  5.0861e-01,\n",
      "          5.3325e-01, -1.7371e-01, -1.3094e-01,  5.9928e-01, -2.0050e+00,\n",
      "         -5.2437e-01, -5.8207e-01,  3.8818e-01, -6.6154e-01, -2.7425e-01,\n",
      "         -2.8446e-02, -6.0517e-01,  6.7119e-02]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Layer 3\n",
      "torch.Size([16, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.2061, -0.7063, -0.4366,  0.2717,  0.0980,  0.2037, -0.5011,\n",
      "          0.1218,  0.4714, -0.8811,  0.0374,  0.1487, -0.2748,  0.2381,  0.0310],\n",
      "        [-0.0652,  0.0000,  1.1066, -0.7974, -0.5848, -3.1836, -0.7115,  0.3770,\n",
      "          0.0313, -0.2822, -0.1164, -1.3150, -0.0788, -0.4942, -0.1021, -0.2665],\n",
      "        [ 0.2309, -0.2055,  0.0000, -1.0610, -1.1011, -0.2757, -0.1736, -0.2457,\n",
      "          0.3728, -0.3123, -0.0955, -0.8336, -0.3817, -0.4922, -0.2491,  0.5534],\n",
      "        [-1.6191, -0.3326,  0.2652,  0.0000, -1.1919, -1.5796, -0.3358,  1.5914,\n",
      "          0.1085,  0.4992,  0.7081,  0.0283, -0.4296, -0.0232, -0.0699, -0.6981],\n",
      "        [ 0.3618,  0.4992, -0.3326, -0.3459,  0.0000, -0.8367, -0.1241, -0.7886,\n",
      "         -0.3497,  0.0432, -0.0457, -0.3576,  0.1105,  0.2469, -0.0415,  0.4657],\n",
      "        [ 0.0739, -0.3355, -0.4605, -0.6675, -0.8635,  0.0000,  0.4844, -2.8432,\n",
      "         -1.7507, -0.8337,  0.3597, -0.3261,  0.2557, -1.8099, -0.8669, -0.0829],\n",
      "        [ 0.4124, -0.8894,  0.9751, -0.3858,  0.0442,  0.4395,  0.0000, -1.8167,\n",
      "         -0.9740,  0.0618,  0.0158, -0.1275,  0.2085, -0.2032,  0.5352, -0.5484],\n",
      "        [-0.8671, -0.6487, -0.3743, -0.8368,  0.0169, -1.0345, -1.3141,  0.0000,\n",
      "         -0.3552, -1.1978, -1.1833,  0.3258, -1.4129, -0.4241, -0.4161, -0.3808],\n",
      "        [-0.3980,  0.2332,  0.3481,  0.1798,  0.0144, -0.5080,  0.1053,  0.2652,\n",
      "          0.0000,  0.1647, -0.9175, -0.0316, -0.5123,  0.1351,  0.2715,  0.3896],\n",
      "        [ 0.2547, -0.4867,  0.0797,  0.6270,  0.5770, -0.6670, -0.0440,  0.5721,\n",
      "         -0.2435,  0.0000, -0.1931, -0.9643,  0.4163, -0.0792, -0.0533, -0.0136],\n",
      "        [-1.7268, -1.9547, -0.9915,  0.6838, -0.3085,  0.1084,  0.6185,  0.9456,\n",
      "         -1.9506, -0.0629,  0.0000,  0.4317,  0.3108,  0.2061, -0.1484, -0.5615],\n",
      "        [ 0.1286, -2.0588, -1.1128,  0.3245, -0.0552, -0.4902,  0.0395,  1.5921,\n",
      "         -0.2173, -0.7231,  0.2755,  0.0000, -1.7840,  0.0563,  0.5210, -0.4938],\n",
      "        [ 0.5669,  0.2390,  0.1089,  0.0175,  0.1239,  0.3571,  0.2729,  0.1734,\n",
      "          0.2059,  0.0586,  0.4949, -0.0082,  0.0000, -0.7122, -2.1094, -0.1305],\n",
      "        [ 0.1657,  0.0696, -0.0633,  0.2142,  0.0416, -2.5529, -0.3397,  0.4431,\n",
      "          0.5103,  0.2060,  0.0243,  0.7477, -0.2643,  0.0000,  0.4101,  0.4401],\n",
      "        [-0.1119, -0.4029, -0.1065,  0.0414, -0.0968, -0.6044, -0.4450,  1.3814,\n",
      "          0.1547, -0.1483, -0.7507,  1.0168, -2.4266,  0.3893,  0.0000,  0.4302],\n",
      "        [ 0.3954,  0.9354, -0.2115, -1.4117,  0.0627, -2.2792, -0.9068,  0.3017,\n",
      "         -0.0796, -0.0540, -3.1429, -0.7099, -0.6061,  0.5636,  0.2452,  0.0000]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Layer 4\n",
      "torch.Size([6, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.7303, -1.3186, -0.9831, -0.9707, -0.3645, -1.2718, -0.5036, -4.4295,\n",
      "         -0.3985, -0.4651, -0.6095, -0.8919, -0.6458, -0.3089, -0.3156, -0.8513],\n",
      "        [-0.3859, -1.6227, -0.8059, -0.9894, -0.3692, -0.8845, -0.3041, -1.5062,\n",
      "         -0.5852, -0.3127, -0.4589, -0.9302, -0.4432, -0.3181, -0.4604, -0.9819],\n",
      "        [-0.5681, -1.6207, -1.0689, -5.8534, -0.9782, -8.4321, -0.6113, -3.3916,\n",
      "         -0.3683, -0.6533, -1.6075, -2.3531, -1.7747, -0.5443, -0.2138, -0.9827],\n",
      "        [-0.5403, -1.9099, -0.9698, -1.0261, -0.3444, -0.8172, -0.2854, -3.5751,\n",
      "         -0.6559, -0.5165, -0.3045, -0.8910, -0.4354, -0.5117, -0.6624, -0.8855],\n",
      "        [-0.7949, -1.7874, -1.3410, -0.7685, -0.2925, -1.0398, -0.5554, -1.0751,\n",
      "         -0.5508, -0.3448, -0.2064, -1.1027, -0.4977, -0.2829, -0.6141, -1.0595],\n",
      "        [-0.6451, -4.0306, -1.5000, -0.8885, -0.4223, -1.5157, -0.6590, -7.1063,\n",
      "         -0.6436, -0.6710, -0.1781, -0.6705, -0.7287, -0.2317, -0.4644, -1.0977]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# check all weights\n",
    "\n",
    "param_list = [*model.parameters()]\n",
    "i = 0\n",
    "for lay in param_list:\n",
    "    i += 1\n",
    "    print(\"Layer {0}\".format(i))\n",
    "    print(lay.shape)\n",
    "    print(lay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0c209",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "314011da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to Recurrent ANN Models/RANN_12.pth\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"Recurrent ANN Models/\"\n",
    "model_name = \"RANN_13.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_dir + model_name)\n",
    "print(\"Saved PyTorch Model State to \" + model_dir + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d75a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207de82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
