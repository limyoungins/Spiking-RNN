{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43712388-c442-4021-bc0f-e898d4323078",
   "metadata": {},
   "source": [
    "# Analogous Recurrent ANN Trainer\n",
    "\n",
    "This notebook can be used to generate and train a recurrent ANN so that the weights can be copied over to a SNN with the same architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bf3b9-a1df-4af0-8122-1af8a12c46d8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf949556-79d1-4121-b739-50c0ca083181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af0fa6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502bd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data and prepare for vector conversion\n",
    "\n",
    "# load data\n",
    "f = open(\"Training Data\\\\train_5500.txt\")\n",
    "data = f.read()\n",
    "\n",
    "# split data into sentences\n",
    "sents = data.split('\\n')\n",
    "\n",
    "# split each sentence into words\n",
    "for i in range(len(sents)):\n",
    "    sents[i] = sents[i].split(' ')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e48c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare word2vector vocabulary (ty chatGPT :) )\n",
    "\n",
    "def read_word_vectors(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, vector_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * vector_size\n",
    "        word_vectors = {}\n",
    "\n",
    "        for _ in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == b' ':\n",
    "                    break\n",
    "                if ch != b'\\n':\n",
    "                    word.append(ch)\n",
    "            word = b''.join(word).decode('utf-8')\n",
    "            vector = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "\n",
    "    return word_vectors\n",
    "\n",
    "def get_word_vector(word, word_vectors):\n",
    "    return word_vectors.get(word)\n",
    "\n",
    "# Load the word vectors\n",
    "word_vectors = read_word_vectors('Word2Vec from Paper\\\\word2vec\\\\trunk\\\\vectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bfbd31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "Word: example\n",
      "Vector: [-2.4861143   2.514228   -2.51763     1.1023496  -1.9080325   0.2741574\n",
      "  1.987924   -0.4649879  -1.3471494   3.144086   -1.9048123   1.5780666\n",
      " -0.08019609 -1.2507837  -2.59727    -0.28834745 -0.7053564  -2.2820096\n",
      " -1.7724434   1.340178    1.0592215   0.5715263  -0.39970756  0.19736235\n",
      "  0.37494832 -0.23648897 -0.5271788  -0.87137115 -0.16628984  0.47225156\n",
      " -2.3885674   0.3888019   1.7539101  -0.90970224  0.7972985  -0.8713628\n",
      " -0.74113584  3.1902182   0.655787   -0.20875123 -0.16770692  2.0293825\n",
      " -0.6267522   0.5787317   1.579219    1.4347987  -0.7990051  -0.19155246\n",
      " -1.1973183   1.641335    2.0438645  -0.9134578  -1.5359813   0.15457954\n",
      " -1.0635711   2.7433052   0.22127318 -1.5445443  -0.5777184  -1.1103141\n",
      "  0.9209189  -1.2365515   0.6689623   0.48781195]\n",
      "\n",
      "(64,)\n",
      "Word: word\n",
      "Vector: [-3.9669743   4.8188896  -2.954842    2.0429592   0.17152616  3.8808334\n",
      "  1.2621093   0.4342894   0.2122565  -1.6349137  -1.7049528  -1.1492262\n",
      "  4.1847644  -0.6373846   1.0669839   1.1486648  -1.1664046  -1.8581262\n",
      " -2.9110322   5.126338    1.3257608   1.2693102  -3.3614407  -1.2822112\n",
      "  5.1168675   0.03109401 -1.8007604  -3.9345162  -2.4310956  -0.53243566\n",
      " -1.5654114   5.0678897   0.30807668 -1.7529594   0.11373261 -1.500378\n",
      "  2.138824    2.2712488  -3.314855    1.1179694  -1.083036    4.7493787\n",
      " -0.26541588  2.1864147   1.2514437  -0.6617835  -0.79325044  0.36480644\n",
      "  1.0892671   2.2959142   1.2033259  -0.95589584 -3.275922    4.2712116\n",
      "  1.0834571   8.340132   -1.8334891  -4.2971954  -1.4262646  -0.5897796\n",
      "  1.8340505  -1.3767011  -2.5821886   0.5281493 ]\n",
      "\n",
      "(64,)\n",
      "Word: vector\n",
      "Vector: [-0.7018949  -1.2055086  -0.99426776  3.5984192  -4.02639     3.3043559\n",
      "  4.7709236   0.11311053  1.5363581   3.7943544  -2.3501387   0.8063334\n",
      "  1.763644    3.2983563  -1.8462338   0.11785336 -5.935734   -3.5553148\n",
      " -2.9720807   3.6282372   3.915432   -1.5889378   0.43966082  0.9580194\n",
      "  0.81886894  0.4643003  -1.9298434  -2.604461   -1.9676477   1.2810096\n",
      " -0.04105073 -0.43293315  0.5917811   0.7420474  -0.36222813 -0.32059428\n",
      "  0.5562149   4.2580957  -0.7984433  -1.7574553  -1.5736797  -0.9554701\n",
      " -1.9561586  -2.089061    5.257163    4.134999   -1.6924216   1.0676143\n",
      " -1.2609527   2.2166708  -4.2431235   1.9631809  -0.24528791 -3.893474\n",
      " -0.22060525 -0.9896199   2.372502   -0.5610109  -3.084976   -3.0171154\n",
      "  3.9851072   8.257224    2.1654563   0.9530224 ]\n",
      "\n",
      "(64,)\n",
      "Word: king\n",
      "Vector: [ 1.7957844  -4.6018395  -0.09757277 -0.4460061  -1.5987397  -2.9004903\n",
      "  3.2889428  -2.1755097   2.53303    -4.811589   -0.58885515 -4.725201\n",
      "  2.6883314   1.7700887   3.7403843   1.7053881   0.7724817   3.8277526\n",
      " -0.16695313 -3.2926173   0.37443164  1.6063944   4.571463   -0.5203922\n",
      "  4.212225    2.5528893   0.08803058  4.0971212  -3.355103    4.4759426\n",
      " -0.14250527  0.09280161 -1.7789161  -2.5967777  -1.6979797  -2.116493\n",
      "  0.4663919  -0.51319855 -1.4585135  -0.22882567  4.1775603   1.000381\n",
      "  1.5282894   0.9049099  -5.1817408   0.6439551   0.18710499  1.720523\n",
      "  1.1604469   0.82973677  3.413264   -0.24482794  2.3681939   2.7713485\n",
      " -2.234261    1.2545004  -3.0336092   2.9394715   1.0767974   3.0911672\n",
      "  4.843916    0.5172443   0.20076227  1.3695639 ]\n",
      "\n",
      "(64,)\n",
      "Word: queen\n",
      "Vector: [ 2.2802994  -4.099436    1.1793872  -2.431531   -2.373581   -0.20208801\n",
      "  6.1322994   2.2811089   0.37748232 -3.466364   -1.0488603  -2.5648284\n",
      "  3.181303    3.5120535  -0.25811362 -0.29654875 -1.8933567   1.5763551\n",
      "  2.9244006  -3.183846    1.6198385   1.0540639   7.024095   -2.5975845\n",
      "  2.43023    -2.4230723  -1.74276     1.0672995  -1.048975    5.5671277\n",
      " -1.8934703  -2.0161183  -3.3639965  -1.5083143  -2.5085785   1.8856598\n",
      " -0.7038164   0.02665352  1.8711231  -2.9604905   4.003653   -2.864039\n",
      " -1.2308829   0.7129145  -0.46878406 -2.268523    0.10041354 -0.87685627\n",
      "  1.0413698  -0.95126265  5.2159863   0.11423327 -1.6311998   0.38125977\n",
      " -2.8785517   2.7560174  -4.7247114  -0.418131    0.46117538  4.7174997\n",
      "  4.633895    1.015396    1.2275217   3.3712578 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test vocabulary\n",
    "\n",
    "# Define a list of words to convert to vectors\n",
    "words = ['example', 'word', 'vector', 'king', 'queen']\n",
    "\n",
    "# Convert words to vectors\n",
    "for word in words:\n",
    "    vector = get_word_vector(word, word_vectors)\n",
    "    print(vector.shape)\n",
    "    if vector is not None:\n",
    "        print(f\"Word: {word}\\nVector: {vector}\\n\")\n",
    "    else:\n",
    "        print(f\"Word: {word} not found in vocabulary.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "402e0cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liamr\\AppData\\Local\\Temp\\ipykernel_13184\\515567485.py:11: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  vecs.append(torch.from_numpy(vec))\n"
     ]
    }
   ],
   "source": [
    "# perform word to vector conversion\n",
    "\n",
    "# perform conversion\n",
    "vec_data = []\n",
    "for sent in sents[:-1]:\n",
    "    vecs = []\n",
    "    vecs.append(sent[0])\n",
    "    for word in sent[1:]:\n",
    "        try:\n",
    "            vec = get_word_vector(word, word_vectors)\n",
    "            vecs.append(torch.from_numpy(vec))\n",
    "        except:\n",
    "            pass\n",
    "    vecs.append(torch.zeros(64))\n",
    "    vec_data.append(vecs)\n",
    "\n",
    "# pad all sentences to length of longest sentence\n",
    "max_len = max([len(sent) for sent in vec_data])\n",
    "vec_data_pad = []\n",
    "for sent in vec_data:\n",
    "    pad_len = max_len - len(sent)\n",
    "    for i in range(pad_len):\n",
    "        sent.append(torch.zeros(64))\n",
    "    vec_data_pad.append(sent)\n",
    "vec_data = vec_data_pad\n",
    "\n",
    "# split into training and test data\n",
    "train_data = vec_data[:5000]\n",
    "test_data = vec_data[5000:-1]\n",
    "\n",
    "# NOTE: first word of each sentence is correct categ. -- last sentence is empty (excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b78ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataSet which can be used with PyTorch DataLoader\n",
    "\n",
    "ans_key = { 'DESC' :  0,\n",
    "            'ENTY' :  1,\n",
    "            'ABBR' :  2,\n",
    "            'HUM'  :  3,\n",
    "            'LOC'  :  4,\n",
    "            'NUM'  :  5 }\n",
    "\n",
    "class QuestionDataset(Dataset):\n",
    "    \"\"\" Question Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        data = list of (list of words -- first word is label)\n",
    "        \"\"\"\n",
    "        self.labels = []\n",
    "        self.sents = []\n",
    "        for sent in data:\n",
    "            lab_val = ans_key[sent[0].split(\":\")[0]]\n",
    "            lab_arr = torch.tensor(lab_val)\n",
    "            self.labels.append(lab_arr)\n",
    "            self.sents.append(sent[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sent = self.sents[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sent, label\n",
    "\n",
    "train_DSet = QuestionDataset(train_data)\n",
    "test_DSet = QuestionDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482a8daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.7112,  3.9187, -2.3859,  1.1708, -2.0570,  1.6660,  2.9410,  0.9965,\n",
      "          1.8875,  2.1013, -1.9081,  1.4046,  2.7684, -1.1823, -1.0634, -0.7583,\n",
      "          0.4377, -3.2159, -1.2020,  1.5030,  0.8661,  0.9120, -0.7257,  0.8150,\n",
      "         -1.4561,  1.0184, -3.1227, -0.7783, -0.3952, -0.3743, -1.6230, -0.2691,\n",
      "          1.9328, -0.7855,  1.0034, -0.2061,  0.9701,  2.3458, -0.4255,  1.8818,\n",
      "         -0.5267,  2.2756,  0.1094, -0.7850,  0.7655, -3.7835, -1.4916, -3.5457,\n",
      "         -1.0294, -0.0488,  0.8952,  0.5742,  0.1470, -0.7216, -0.5818,  2.2442,\n",
      "         -1.0479, -1.2184, -1.7076,  0.3324,  0.3778, -1.3552,  1.4898,  2.8329]]), tensor([[-0.1255, -0.4126, -1.5036,  0.5658, -1.5322, -0.3062,  2.7857,  0.9233,\n",
      "          0.3195, -0.4306, -2.1697,  0.2383, -0.0840, -0.7352,  1.3612, -0.5537,\n",
      "         -0.4857, -0.1546, -1.3385, -0.5487, -0.7021,  1.0826, -0.8593,  1.2547,\n",
      "         -1.8827,  0.5152, -2.4523, -0.1934, -1.8966, -1.3499,  1.0865, -0.1157,\n",
      "         -1.6995, -0.4025,  1.0100,  0.8994, -0.7290, -0.4267, -1.0596,  1.2209,\n",
      "         -0.7207,  0.3300,  0.3862, -0.9140,  0.4267, -0.3296,  1.6180, -2.5494,\n",
      "         -0.4436, -1.1370, -0.2836,  1.1734,  1.3698,  0.2817,  0.7277,  1.6965,\n",
      "         -1.6160, -0.3719, -1.2655,  1.1716, -1.4072, -0.3927,  0.8941,  0.8323]]), tensor([[ 5.0867e+00, -5.6811e+00,  1.0260e+00, -7.7513e-02,  8.3241e-01,\n",
      "          1.8181e+00, -7.5181e-01, -6.9512e-01, -1.0106e+00, -1.9495e+00,\n",
      "         -2.0785e+00, -8.5342e-01,  5.7848e-01, -5.7674e-01,  1.3226e+00,\n",
      "          7.6725e-01,  7.2153e-01,  4.5867e+00,  1.2998e+00, -3.0795e-01,\n",
      "          1.6018e+00, -2.1377e+00, -1.3543e+00,  1.1258e+00,  1.0474e+00,\n",
      "          1.4048e+00,  1.3730e+00, -9.0983e-01,  3.3841e+00, -1.3204e+00,\n",
      "          5.4364e-01,  6.4220e-01, -2.1381e+00, -1.3716e+00, -2.2054e+00,\n",
      "          1.9349e+00,  2.2844e+00, -2.1739e+00,  1.5685e+00,  1.8603e-01,\n",
      "          2.1605e+00, -4.0489e-01, -1.4935e+00,  1.6408e+00,  2.3671e-03,\n",
      "         -4.0034e-01,  2.3765e+00, -8.9457e-01,  3.5255e-01,  3.7269e-01,\n",
      "          6.0329e-01, -3.6403e-01, -2.1499e+00,  1.0936e+00, -6.0951e-03,\n",
      "         -1.6029e+00, -1.4292e+00,  1.5668e+00,  5.9511e-01,  7.8712e-01,\n",
      "          1.1983e+00,  2.8693e+00, -9.0018e-01, -9.7079e-01]]), tensor([[-1.4143,  0.0992, -2.3108, -0.5830, -1.0944, -0.3193,  1.9195,  0.2699,\n",
      "          1.2051, -1.0585, -2.2606,  0.1206, -1.9612, -1.0926, -0.2506, -0.0557,\n",
      "         -0.6838,  0.1701, -2.3205, -2.0101,  0.1891,  1.1598, -1.2154,  0.8956,\n",
      "         -2.3289,  0.7250, -2.0758, -0.4225, -1.2188, -1.7325,  0.6240,  0.6788,\n",
      "         -0.3515,  0.1365, -1.1027,  0.5103,  0.8050, -0.1385, -0.0412,  0.2296,\n",
      "         -0.2874, -1.0682, -0.4848, -1.2418,  0.4459,  0.6206,  1.7431, -1.8604,\n",
      "         -0.4546, -1.5489,  0.2660, -0.0554,  0.7522, -0.0210,  0.6118,  1.2925,\n",
      "         -0.5698,  0.2403, -1.0763,  0.9770, -0.7178, -0.5330,  1.1980,  1.9875]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n",
      "tensor([3])\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_DSet, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_DSet, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256c456",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a981bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (ff): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=48, bias=False)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (rnn): RNN(48, 16, bias=False)\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=6, bias=False)\n",
      "    (1): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" Builds recurrent neural network model \"\"\"\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(64, 48, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.rnn = nn.RNN(48, 16, nonlinearity='relu', bias=False)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(16, 6, bias=False),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, q):\n",
    "        \"\"\" Implements feed-forward then recurrent layer \"\"\"\n",
    "        ff_q = []\n",
    "        for word in q:\n",
    "            ff_q.append(self.ff(word))\n",
    "        ff_q = torch.stack(ff_q)\n",
    "        h_N = Variable(torch.zeros(1, 16)).to(device)\n",
    "        rnn_out = Variable(torch.zeros(1, 16)).to(device)\n",
    "        for word in ff_q:\n",
    "            if not torch.all(word.eq(0)):\n",
    "                rnn_out, h_N = self.rnn(word, h_N)\n",
    "            else:\n",
    "                break\n",
    "        output = self.out(rnn_out)\n",
    "        return output\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a554826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model\n",
    "\n",
    "param_list = [*model.parameters()]\n",
    "len(param_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f51ab5",
   "metadata": {},
   "source": [
    "## Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "873219a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss function and optimizer\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# define training routine\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    flag = False\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = torch.stack(X).to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error and optimize\n",
    "        optimizer.zero_grad()\n",
    "        \"\"\" now handled inside model -- keep to count empties\n",
    "        for word in X:\n",
    "            if torch.all(word.eq(0)):\n",
    "                print(\"FLAG!\")\n",
    "                print(X)\n",
    "                flag = True\n",
    "                break\n",
    "            else:\n",
    "                break\n",
    "        \"\"\"\n",
    "        if not flag:\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # clamp diagonal hidden-hidden weights of RNN layer to 0\n",
    "            model.rnn._parameters['weight_hh_l0'].data.diagonal().clamp_(min=0, max=0)\n",
    "\n",
    "            # print loss and accuracy at selected iterations\n",
    "            if batch % 1000 == 0:\n",
    "                loss, current = loss.item(), (batch + 1)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        else:\n",
    "            flag = False\n",
    "            pass\n",
    "\n",
    "# define test routine\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = torch.stack(X).to(device), y.to(device) #torch.FloatTensor(y).to(device)\n",
    "            pred = model(X)\n",
    "            target = y#.argmax(1)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f2121fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e3ddd2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.020716  [    1/ 5000]\n",
      "loss: 0.000030  [ 1001/ 5000]\n",
      "loss: 0.000004  [ 2001/ 5000]\n",
      "loss: 0.196963  [ 3001/ 5000]\n",
      "loss: 0.000000  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 1.267711 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.000000  [    1/ 5000]\n",
      "loss: 0.001946  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.007449  [ 3001/ 5000]\n",
      "loss: 0.001513  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.379080 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.975907  [    1/ 5000]\n",
      "loss: 0.003614  [ 1001/ 5000]\n",
      "loss: 0.044577  [ 2001/ 5000]\n",
      "loss: 0.032394  [ 3001/ 5000]\n",
      "loss: 0.000000  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.374145 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.620859  [    1/ 5000]\n",
      "loss: 0.016005  [ 1001/ 5000]\n",
      "loss: 0.000051  [ 2001/ 5000]\n",
      "loss: 0.000365  [ 3001/ 5000]\n",
      "loss: 0.011519  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.477798 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.035386  [    1/ 5000]\n",
      "loss: 0.020244  [ 1001/ 5000]\n",
      "loss: 0.000014  [ 2001/ 5000]\n",
      "loss: 0.030098  [ 3001/ 5000]\n",
      "loss: 0.000463  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.419507 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.000000  [    1/ 5000]\n",
      "loss: 0.002678  [ 1001/ 5000]\n",
      "loss: 0.000436  [ 2001/ 5000]\n",
      "loss: 0.000094  [ 3001/ 5000]\n",
      "loss: 2.522235  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.473834 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.000067  [    1/ 5000]\n",
      "loss: 0.045461  [ 1001/ 5000]\n",
      "loss: 0.000327  [ 2001/ 5000]\n",
      "loss: 0.657470  [ 3001/ 5000]\n",
      "loss: 0.004053  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 1.475717 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.017289  [    1/ 5000]\n",
      "loss: 0.000002  [ 1001/ 5000]\n",
      "loss: 0.223175  [ 2001/ 5000]\n",
      "loss: 0.000010  [ 3001/ 5000]\n",
      "loss: 0.000000  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.518605 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.001722  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.012075  [ 2001/ 5000]\n",
      "loss: 0.003281  [ 3001/ 5000]\n",
      "loss: 0.033959  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.552490 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000000  [    1/ 5000]\n",
      "loss: 0.000118  [ 1001/ 5000]\n",
      "loss: 0.226070  [ 2001/ 5000]\n",
      "loss: 0.025886  [ 3001/ 5000]\n",
      "loss: 0.000389  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.575340 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.000000  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.100451  [ 3001/ 5000]\n",
      "loss: 0.039578  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 1.591276 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.002628  [    1/ 5000]\n",
      "loss: 0.005151  [ 1001/ 5000]\n",
      "loss: 1.912973  [ 2001/ 5000]\n",
      "loss: 0.000177  [ 3001/ 5000]\n",
      "loss: 0.488496  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.677780 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.055351  [    1/ 5000]\n",
      "loss: 0.045352  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.033112  [ 3001/ 5000]\n",
      "loss: 0.065884  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.658811 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.000000  [    1/ 5000]\n",
      "loss: 0.001281  [ 1001/ 5000]\n",
      "loss: 0.746245  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 0.000000  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.692672 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.040861  [    1/ 5000]\n",
      "loss: 1.024590  [ 1001/ 5000]\n",
      "loss: 1.733644  [ 2001/ 5000]\n",
      "loss: 2.484078  [ 3001/ 5000]\n",
      "loss: 0.000000  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.724585 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.000246  [    1/ 5000]\n",
      "loss: 1.424215  [ 1001/ 5000]\n",
      "loss: 0.000023  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 0.000001  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.822729 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.000000  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.049604  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 0.464723  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.776956 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.005828  [    1/ 5000]\n",
      "loss: 0.073244  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.076379  [ 3001/ 5000]\n",
      "loss: 0.000004  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.762489 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.004361  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.000194  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 0.000002  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 1.802239 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.000607  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.008053  [ 2001/ 5000]\n",
      "loss: 0.000000  [ 3001/ 5000]\n",
      "loss: 0.012956  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 1.870679 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.791759  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.003320  [ 2001/ 5000]\n",
      "loss: 0.174886  [ 3001/ 5000]\n",
      "loss: 0.001347  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.937485 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.007328  [    1/ 5000]\n",
      "loss: 0.050199  [ 1001/ 5000]\n",
      "loss: 0.000274  [ 2001/ 5000]\n",
      "loss: 0.036699  [ 3001/ 5000]\n",
      "loss: 0.000143  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.911639 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.000000  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.000121  [ 2001/ 5000]\n",
      "loss: 0.000533  [ 3001/ 5000]\n",
      "loss: 0.059585  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 1.962271 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.038351  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.009787  [ 3001/ 5000]\n",
      "loss: 0.000026  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.983421 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.003021  [    1/ 5000]\n",
      "loss: 0.000000  [ 1001/ 5000]\n",
      "loss: 0.000000  [ 2001/ 5000]\n",
      "loss: 0.000018  [ 3001/ 5000]\n",
      "loss: 0.000470  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 1.933041 \n",
      "\n",
      "Done!\n",
      "\n",
      "Total epochs: 75\n"
     ]
    }
   ],
   "source": [
    "# perform training and test performance over epochs\n",
    "\n",
    "epochs = 25\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "total_epochs += epochs\n",
    "print(\"\\nTotal epochs: {0}\".format(total_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfc45f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case training takes a wrong turn\n",
    "temp_model_backup = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6b18c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "torch.Size([48, 64])\n",
      "Parameter containing:\n",
      "tensor([[-0.2471, -0.3934,  0.2969,  ...,  0.6282,  0.0414,  0.3432],\n",
      "        [-0.3621,  0.1632, -0.0486,  ..., -0.3463,  0.4843, -0.1778],\n",
      "        [ 0.3735,  0.1269, -0.5631,  ...,  0.2813,  0.1237, -0.4632],\n",
      "        ...,\n",
      "        [-0.1461,  0.2327,  0.3336,  ..., -0.2379, -0.3014, -0.1029],\n",
      "        [-0.0546, -0.3894,  0.1276,  ...,  0.5659,  0.1009, -0.2041],\n",
      "        [ 0.0088, -0.0919, -1.1074,  ..., -0.1097,  0.4622, -0.0040]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Layer 2\n",
      "torch.Size([16, 48])\n",
      "Parameter containing:\n",
      "tensor([[ 1.8954e-01,  6.0265e-01, -1.2637e-01,  1.4731e-01,  7.2966e-01,\n",
      "          2.3048e-03,  1.3296e-01, -2.8251e-01,  6.5898e-01,  5.1831e-01,\n",
      "          1.8277e-01,  4.5556e-01, -2.8265e-01,  6.8253e-01, -7.9641e-02,\n",
      "          6.7937e-03, -1.6428e-01,  2.3245e-01,  2.3486e-01, -1.2748e-01,\n",
      "          3.7089e-01, -4.1698e-01,  1.4117e-01, -2.5603e-01,  2.7422e-01,\n",
      "          4.7966e-01,  4.5276e-02, -4.3769e-02,  5.0264e-01,  7.8587e-01,\n",
      "         -3.0462e-01,  3.6579e-01,  4.3083e-01, -1.0786e-01, -3.7824e-02,\n",
      "          1.5143e-01,  1.9350e-02, -9.0236e-01, -9.8034e-02, -1.1291e+00,\n",
      "         -3.3295e-01,  4.6251e-01,  1.8693e-01,  1.0277e-02,  2.8930e-01,\n",
      "          2.4319e-01,  1.6913e-01, -3.2852e-01],\n",
      "        [-4.3390e-01,  1.1153e-02,  2.4931e-01,  3.1308e-01,  1.0736e-02,\n",
      "          7.0991e-01,  3.7196e-01,  3.2653e-01,  6.7328e-02,  5.7486e-01,\n",
      "         -1.4623e-01,  3.0461e-01,  4.1874e-01, -2.8699e-01,  1.8420e-01,\n",
      "         -3.7957e-01,  6.3967e-01, -4.6669e-01,  1.6918e-01, -6.5827e-01,\n",
      "         -1.5411e-01,  7.1384e-01, -3.2011e-01,  2.6278e-01,  4.1587e-01,\n",
      "          1.7106e-02,  2.3389e-01, -3.1167e-01,  3.8823e-01, -2.1712e-01,\n",
      "         -2.1358e-01,  6.7142e-01,  5.6860e-01,  1.8539e-01, -2.9489e-01,\n",
      "         -2.1521e-01,  2.2608e-01,  8.2704e-02,  8.7822e-02,  1.0839e-01,\n",
      "         -1.0108e-01,  7.4538e-01,  3.8547e-01, -2.2147e-02, -6.3305e-02,\n",
      "         -3.3399e-01, -1.8680e-01, -8.9765e-02],\n",
      "        [-3.2638e-02,  3.8035e-01,  9.4303e-02,  1.0761e-01, -4.5625e-03,\n",
      "         -4.4241e-01, -5.5742e-01,  3.5030e-02,  5.1641e-01,  4.5490e-01,\n",
      "          7.8981e-02, -1.5867e-01,  1.3264e-02, -1.3302e-01,  2.1136e-01,\n",
      "          3.0142e-01,  3.6172e-02,  6.8546e-01, -3.0701e-01, -2.0846e-01,\n",
      "         -5.8696e-02, -7.8695e-01, -4.6989e-01,  8.7078e-01,  3.3875e-01,\n",
      "          3.0326e-01, -4.5553e-01, -3.0393e-01,  2.8208e-01,  2.7682e-01,\n",
      "          4.5625e-01, -3.3881e-01,  1.2532e-01, -3.1880e-01,  4.6926e-01,\n",
      "          3.0151e-01, -5.5041e-02, -5.2107e-02, -1.1907e-01, -1.0502e-01,\n",
      "         -7.6543e-02, -3.4267e-01,  1.0729e-01, -5.5949e-01,  1.9740e-01,\n",
      "         -2.7590e-01, -5.4118e-02,  8.8725e-02],\n",
      "        [ 4.5693e-01, -4.9000e-01, -2.1074e-02,  1.6324e-01,  2.4637e-01,\n",
      "         -3.9592e-01,  4.2686e-01,  2.7955e-01, -6.8419e-02,  3.6038e-01,\n",
      "          3.1044e-01, -4.1639e-01, -4.6485e-01,  2.6223e-02, -6.0146e-02,\n",
      "          3.7902e-01, -2.4419e-01,  1.9001e-02, -2.3792e-01,  1.5422e-01,\n",
      "          6.0622e-01,  8.1304e-02, -3.3777e-01, -2.2463e-01,  3.7233e-02,\n",
      "         -5.4628e-01,  1.2774e-01,  4.7708e-01, -9.8938e-01,  1.1410e-02,\n",
      "          1.5056e-01, -2.5409e-01, -2.8856e-02,  5.6661e-02,  3.7045e-02,\n",
      "          3.1228e-01,  5.2431e-02,  3.4898e-01, -4.9321e-01, -1.1471e-01,\n",
      "          3.1279e-02, -8.3162e-02,  5.9735e-02,  8.1172e-02,  2.9042e-01,\n",
      "          2.9530e-01,  3.7142e-01, -1.6964e-01],\n",
      "        [-6.1341e-01,  1.3883e-01,  2.2284e-01, -2.6989e-01,  4.1210e-01,\n",
      "          3.5224e-01,  4.0204e-01,  1.7925e-02,  1.3021e-01, -2.5361e-01,\n",
      "         -1.8705e-01, -5.2486e-01, -2.1119e-01, -7.3841e-01,  2.1589e-02,\n",
      "          2.0233e-01,  4.1118e-01, -4.2194e-01,  8.4564e-01,  1.1453e-01,\n",
      "         -3.3252e-01, -5.4149e-01, -4.7395e-01,  4.7061e-01, -2.6188e-01,\n",
      "         -1.2514e-01,  6.3843e-01, -2.0222e-02, -1.1217e-01,  5.9081e-01,\n",
      "         -2.1103e-01,  1.8953e-01,  1.6685e-01,  7.2501e-03,  1.9901e-01,\n",
      "          1.8612e-02,  7.9333e-01,  2.8292e-02, -2.9478e-01, -3.4309e-01,\n",
      "          2.5009e-02,  7.0662e-02,  2.7523e-01, -1.2569e-01,  1.9968e-01,\n",
      "         -2.8175e-01,  1.4589e-02, -4.0395e-01],\n",
      "        [ 3.7015e-03, -9.2846e-01,  3.3055e-01,  4.7162e-02, -1.3820e-01,\n",
      "         -1.0922e-01,  3.5964e-01,  3.4871e-01, -6.3476e-02,  4.0117e-02,\n",
      "         -2.0191e-01, -8.8937e-01, -4.9364e-01,  1.3326e-01, -6.6274e-02,\n",
      "          1.1198e-01, -4.0432e-01,  1.0973e-01,  1.3841e-01, -2.3926e-01,\n",
      "         -5.0392e-01, -3.1362e-03,  1.2994e-01,  3.2989e-01,  7.8205e-02,\n",
      "         -9.1979e-01,  6.0796e-01, -3.1402e-01,  2.0551e-01, -4.0902e-02,\n",
      "          1.5364e-02, -5.9503e-01, -3.2376e-01,  5.2959e-01,  9.1245e-02,\n",
      "          4.5610e-01, -1.3099e-01,  4.8128e-01,  2.9859e-01,  3.0544e-02,\n",
      "          9.9911e-01, -3.8730e-01, -9.9391e-02,  1.6074e-01, -4.0800e-01,\n",
      "          1.9020e-01, -3.3126e-02, -3.1402e-01],\n",
      "        [-7.1522e-03, -2.7012e-01, -1.4569e-01, -6.7778e-01, -1.8421e-01,\n",
      "         -1.5515e-01, -4.3428e-01,  4.0834e-01,  5.0009e-01,  1.9901e-01,\n",
      "         -9.3473e-01,  3.6374e-01, -3.7755e-04,  6.9668e-02, -1.8890e-02,\n",
      "          3.3020e-01,  2.0235e-01,  1.7133e-01, -9.5641e-02, -1.4059e-01,\n",
      "          4.5972e-02, -5.0258e-01, -6.2828e-01,  1.3514e-01, -1.8369e-01,\n",
      "          2.2294e-01,  3.0207e-01,  2.3859e-01, -2.5829e-01,  3.7836e-01,\n",
      "         -5.5519e-01,  1.6156e-01,  1.9752e-02, -2.0191e-01,  2.0297e-01,\n",
      "          1.6298e-01, -7.8468e-01, -2.6661e-01,  1.1363e-01,  6.5900e-01,\n",
      "         -3.5708e-01,  5.1788e-01,  2.8852e-01, -6.1445e-01,  4.3018e-01,\n",
      "          6.2224e-01, -2.7434e-01, -2.5165e-01],\n",
      "        [ 4.5574e-01, -1.9633e-01, -4.9554e-03, -1.4497e-01, -4.2631e-01,\n",
      "          1.6806e-01,  3.4002e-02,  1.2864e-02, -2.3130e-01, -3.1341e-01,\n",
      "          5.0434e-02, -2.1449e-02, -1.2072e-01,  2.0371e-01, -9.0808e-01,\n",
      "          1.5645e-01, -3.5826e-01,  2.7241e-01, -2.1164e-01, -3.9608e-01,\n",
      "          3.4146e-01,  1.3565e-01, -3.0786e-01, -8.0512e-01,  8.2730e-01,\n",
      "         -1.4549e-01,  2.4108e-01, -2.4196e-01,  6.3308e-01, -5.3941e-01,\n",
      "          7.8555e-02,  6.6515e-01,  1.0651e-01, -1.3238e-01,  2.6382e-02,\n",
      "          1.6927e-01, -3.5787e-01,  1.8527e-01, -1.6707e-01, -1.4875e-01,\n",
      "         -3.3312e-01,  5.9155e-01,  6.0950e-01, -3.3808e-01, -9.1003e-02,\n",
      "         -5.5733e-01,  4.9993e-01, -2.4776e-01],\n",
      "        [-4.7840e-01,  2.2109e-01,  3.9519e-01,  5.9487e-01, -4.2168e-01,\n",
      "         -2.5200e-01, -1.3506e-01,  3.9905e-01,  2.6043e-01, -2.5288e-01,\n",
      "         -8.5583e-02,  2.2020e-01, -9.5269e-01,  5.8690e-01,  5.4366e-01,\n",
      "         -1.0977e+00,  3.8774e-02,  5.7180e-01, -9.7626e-02,  6.3060e-02,\n",
      "         -2.7885e-01, -1.0481e+00, -6.6829e-01, -5.5962e-01, -6.1266e-02,\n",
      "         -8.0766e-02,  1.0275e+00,  1.0356e-02, -3.9871e-02,  5.9141e-01,\n",
      "         -5.2180e-01, -4.6559e-01,  7.4603e-01, -7.6761e-01,  2.8884e-01,\n",
      "          4.0658e-01, -8.3872e-03, -3.2008e-01, -3.2701e-01,  1.3912e-01,\n",
      "         -3.5044e-01, -1.2345e+00,  7.4465e-01, -7.5385e-01, -6.1628e-02,\n",
      "          3.1191e-01,  2.1703e-02,  2.1001e-01],\n",
      "        [ 1.2545e-01,  4.1076e-01, -2.7587e-01,  1.5874e-01,  2.8677e-01,\n",
      "         -1.3902e-01, -4.2754e-02, -4.1979e-01,  3.5015e-02,  7.1163e-02,\n",
      "          3.7488e-01,  3.0141e-01, -1.8801e-01,  3.0830e-01, -1.9437e-01,\n",
      "         -5.6537e-01, -2.0665e-01, -3.2302e-01,  6.9149e-01, -6.8715e-01,\n",
      "         -2.4849e-01,  3.7651e-01, -3.6198e-01, -2.4886e-01, -2.7609e-01,\n",
      "          3.8273e-01, -8.5498e-03, -1.1398e-02,  6.3018e-01,  2.1929e-01,\n",
      "          1.6821e-01, -2.5732e-01, -3.9680e-01,  3.0906e-01,  2.4793e-01,\n",
      "          1.7376e-01, -2.3030e-01, -6.5043e-02, -6.9103e-01,  1.2865e-02,\n",
      "          4.1593e-02,  1.3542e-01,  1.1955e-01,  1.6371e-01,  3.1783e-01,\n",
      "          1.6718e-01,  2.6557e-02, -1.3583e-01],\n",
      "        [-1.7778e-01, -3.8581e-01, -5.8360e-01,  4.0248e-01,  5.0265e-01,\n",
      "          2.6111e-01, -1.2463e-01,  6.0684e-02, -2.2995e-01,  4.0848e-01,\n",
      "          6.9392e-01,  2.0108e-01,  2.8930e-01,  2.7308e-01, -1.2386e-01,\n",
      "         -2.3972e-01,  5.1449e-01, -3.4913e-01,  7.1378e-02, -1.3794e-02,\n",
      "          6.6977e-03, -1.4562e-01,  1.0938e-02, -1.7954e-01, -4.5794e-01,\n",
      "          1.3153e-02,  4.1174e-01,  4.8441e-01, -1.2229e-01,  1.0422e-02,\n",
      "         -6.6636e-01,  3.0767e-01, -3.0147e-01, -1.7252e-01, -5.1777e-01,\n",
      "          8.9819e-01,  4.3005e-01,  6.7228e-01, -3.6996e-01,  5.8775e-02,\n",
      "         -3.4618e-01, -3.6904e-01,  2.2153e-01, -1.9262e-01,  1.1006e-01,\n",
      "          2.0313e-01,  2.2376e-01,  4.6236e-01],\n",
      "        [ 7.8939e-01, -1.5711e-01,  2.2758e-02, -2.3764e-01, -5.4962e-02,\n",
      "         -4.9333e-01,  2.8990e-01,  1.7496e-01,  3.9341e-01, -2.7332e-01,\n",
      "         -5.1498e-01,  1.1258e-01,  3.2270e-01,  2.5876e-01, -1.2112e+00,\n",
      "          1.7483e-01,  4.7901e-01,  2.8316e-01,  3.9001e-01, -4.8398e-02,\n",
      "          2.5596e-01, -1.4331e-01,  4.5591e-01, -9.8454e-02,  5.2604e-01,\n",
      "         -1.5619e-01,  2.3666e-01,  4.4961e-01,  6.1105e-01,  2.6595e-01,\n",
      "         -1.2047e-01, -6.2587e-02,  3.2657e-01,  3.1505e-01,  1.0064e-01,\n",
      "          4.6576e-02,  1.8271e-01, -6.1995e-01, -4.8793e-01,  2.2924e-01,\n",
      "         -1.9774e-01,  2.4689e-01,  2.1153e-01, -4.5418e-01, -9.0146e-02,\n",
      "         -1.2311e+00, -1.9691e-01, -3.2980e-01],\n",
      "        [ 3.1713e-01, -3.3241e-01,  6.5853e-01, -3.4487e-01,  3.4493e-01,\n",
      "         -2.8949e-01, -1.0720e+00,  2.5085e-01, -4.5121e-01,  2.0967e-01,\n",
      "          1.5327e-01, -1.8497e-01,  1.2853e-02, -6.8901e-02, -1.7020e-01,\n",
      "          3.9620e-01, -1.0331e+00,  8.2173e-02,  5.8525e-01,  2.9496e-01,\n",
      "          3.9902e-01, -3.4820e-01,  1.5432e-01, -2.1379e-02,  2.2300e-01,\n",
      "         -8.3689e-01, -1.1257e-01,  2.7857e-01,  5.1624e-01,  2.6506e-01,\n",
      "         -1.0992e-01, -3.2452e-01, -4.6404e-01,  4.4629e-02,  6.7231e-01,\n",
      "          2.0046e-02,  3.2056e-01,  1.1005e-01,  4.1820e-01,  5.7790e-01,\n",
      "         -2.5697e-01, -1.3096e-02,  6.4293e-02, -6.5313e-01, -9.4248e-01,\n",
      "          7.1313e-02, -6.3043e-01,  2.0121e-01],\n",
      "        [-2.1409e-01, -2.0796e-01,  4.2497e-01,  7.2122e-01, -3.2217e-01,\n",
      "          8.1938e-01, -7.0214e-02, -8.7238e-02,  1.1856e-01, -6.6318e-01,\n",
      "          2.8422e-01,  1.8852e-01,  3.2545e-02, -5.9909e-01,  1.0196e-01,\n",
      "          6.1273e-01,  2.2343e-01, -8.1030e-01,  2.6057e-02, -1.7089e-01,\n",
      "          2.8638e-01, -6.8126e-02, -3.1455e-01,  1.8966e-01,  4.5975e-01,\n",
      "          1.6773e-01,  7.5729e-03,  4.5682e-01,  5.6905e-01, -5.3133e-01,\n",
      "          1.1562e-01,  5.0373e-01,  3.4304e-01,  1.5086e-01,  2.9377e-01,\n",
      "         -2.4570e-01, -2.7519e-01,  2.1940e-01, -4.1033e-01, -1.0373e-01,\n",
      "         -4.9063e-01, -7.6555e-02, -3.9457e-01, -2.6119e-01, -9.2948e-01,\n",
      "         -3.3365e-01, -5.9319e-01,  3.1142e-02],\n",
      "        [ 6.4153e-02, -8.1276e-01,  3.6793e-01,  2.2316e-01,  1.8793e-01,\n",
      "          3.9232e-02, -2.1149e-01, -3.3576e-01, -3.3482e-01,  1.2916e-01,\n",
      "         -1.7506e-01, -5.6045e-01,  2.7033e-01, -8.6302e-01,  1.8358e-01,\n",
      "         -1.5927e-01,  2.8669e-01,  9.9474e-02, -7.2549e-02, -2.2042e-01,\n",
      "          3.8336e-01,  1.3655e-01,  5.2695e-01,  1.7235e-01,  3.3118e-01,\n",
      "         -8.0737e-01,  4.7744e-01,  4.4288e-01, -2.5186e-01, -4.4974e-01,\n",
      "         -5.5209e-01, -6.1342e-01, -3.7417e-04,  3.2214e-01, -1.7392e-01,\n",
      "          4.2179e-01,  2.5510e-01,  3.4636e-01, -5.0171e-01,  4.0056e-01,\n",
      "         -8.6191e-01,  1.6543e-01, -1.2683e-01, -4.3528e-01, -5.7357e-01,\n",
      "          7.3405e-02,  6.4939e-01,  2.6297e-01],\n",
      "        [-3.2983e-01, -4.6479e-01, -1.2282e-01, -5.8913e-02, -3.3265e-02,\n",
      "          3.0674e-01,  5.8597e-02,  1.4514e-01,  3.0369e-02, -3.2593e-01,\n",
      "         -2.7523e-01,  3.5231e-02, -6.6947e-01,  2.8183e-01,  4.0262e-01,\n",
      "         -1.6065e-01, -2.3132e-01,  6.5080e-01, -2.4295e-01, -1.1244e+00,\n",
      "          8.9316e-02,  1.8276e-02, -4.8393e-01,  4.2476e-01, -4.3261e-01,\n",
      "          2.5179e-01,  1.1607e-01,  1.1162e-01, -3.5901e-01, -6.2359e-01,\n",
      "          2.7110e-01,  4.8343e-01,  3.6824e-01,  6.0487e-02, -1.1521e-01,\n",
      "          1.6660e-01, -2.9584e-01,  5.1086e-01, -4.8371e-01, -4.8788e-01,\n",
      "          6.3752e-03,  1.5901e-01, -5.3922e-01, -7.3290e-01,  1.7695e-01,\n",
      "          6.8221e-02,  4.2800e-03, -9.0658e-01]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Layer 3\n",
      "torch.Size([16, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000e+00, -6.4251e-01, -1.0974e+00, -9.3479e-01, -9.7985e-01,\n",
      "         -1.3456e-01, -1.3996e+00,  1.7854e-01,  3.1176e-01, -3.5277e-01,\n",
      "         -1.0170e+00, -4.6108e-02, -7.0970e-01, -7.3124e-01, -1.8754e+00,\n",
      "         -9.4034e-01],\n",
      "        [-2.6509e-01,  0.0000e+00, -3.1561e-01, -1.6996e+00,  6.0101e-01,\n",
      "         -1.1578e-01, -4.8270e-01, -4.4891e-01,  1.8994e-02, -4.2785e-01,\n",
      "          2.6339e-01, -4.3480e-02, -3.0857e-01,  5.4127e-01,  1.1410e-01,\n",
      "         -9.9276e-02],\n",
      "        [ 6.9795e-01, -4.9220e-01,  0.0000e+00,  4.1829e-01, -2.1531e-01,\n",
      "          1.6477e-01, -1.8632e-03,  5.6174e-02,  2.0373e-01,  4.1946e-01,\n",
      "         -1.1435e-01,  7.9362e-01, -1.8509e-01,  3.9187e-01, -8.4472e-01,\n",
      "          3.3499e-02],\n",
      "        [ 9.8226e-01, -8.6978e-01,  4.6473e-01,  0.0000e+00, -7.7598e-02,\n",
      "         -2.0726e-01, -2.5326e-01, -1.1948e+00,  8.8881e-01,  3.2605e-01,\n",
      "          6.4432e-01, -1.0569e+00,  2.6725e-01, -3.2855e-01,  6.5642e-01,\n",
      "         -1.4073e+00],\n",
      "        [-8.9020e-01,  3.7174e-01, -1.9703e-01, -2.2829e-01,  0.0000e+00,\n",
      "          3.4858e-01,  1.4426e-01, -3.5280e-01,  6.7976e-04, -7.5891e-01,\n",
      "          2.3015e-01,  1.5363e-01,  3.6270e-01,  8.5518e-03,  6.6783e-02,\n",
      "          3.3486e-01],\n",
      "        [ 1.1597e-01,  2.4665e-01,  3.8781e-01,  9.2783e-02, -1.4025e-02,\n",
      "          0.0000e+00, -2.2277e-01, -2.8000e-01, -2.6737e-02, -3.8397e-01,\n",
      "         -6.6538e-01,  1.0561e+00,  5.7140e-01, -4.6867e-01,  6.1417e-02,\n",
      "          3.4609e-01],\n",
      "        [ 1.0418e-01, -3.2011e-02,  6.4288e-02,  8.0120e-02,  7.9578e-02,\n",
      "          2.8674e-01,  0.0000e+00,  2.1718e-01,  7.8690e-01, -2.7324e-03,\n",
      "         -1.7196e-01,  6.1764e-01,  5.5784e-02, -1.3695e-01, -2.5972e-01,\n",
      "          5.2581e-01],\n",
      "        [ 3.9097e-01, -4.5235e-01,  4.4431e-02, -7.7417e-01, -1.3886e+00,\n",
      "         -1.7001e-01,  3.7017e-01,  0.0000e+00, -5.4102e-01,  5.1117e-01,\n",
      "         -8.4233e-01,  1.1495e+00, -5.9414e-02, -1.0445e-01, -2.3935e-01,\n",
      "          5.7500e-01],\n",
      "        [-3.2365e-02, -7.0899e-02, -1.3405e-01,  2.6023e-01, -1.8432e-01,\n",
      "         -6.7573e-03, -4.5129e-01,  2.4367e-01,  0.0000e+00, -3.6489e-01,\n",
      "         -4.5891e-01,  1.8282e-01, -3.6803e-01,  1.6092e-01, -6.9641e-01,\n",
      "         -5.1547e-01],\n",
      "        [ 1.5114e+00,  4.9332e-03,  3.1685e-01,  3.0216e-01, -9.0284e-01,\n",
      "          4.6498e-01,  2.9693e-01,  4.8467e-01,  4.9381e-01,  0.0000e+00,\n",
      "          3.3916e-02,  8.9372e-01, -5.3757e-01,  2.4755e-01, -1.1251e+00,\n",
      "         -2.1458e-01],\n",
      "        [ 6.1058e-01,  2.3416e-01, -3.5856e-02,  5.0420e-01, -1.2112e-01,\n",
      "         -1.2901e-01, -4.9395e-01, -6.3916e-01,  4.8821e-01, -2.1095e-01,\n",
      "          0.0000e+00, -7.7082e-01, -2.4532e-01,  3.6140e-01,  8.9669e-01,\n",
      "         -8.5798e-01],\n",
      "        [ 1.2569e+00, -1.1024e-01, -2.9386e-01, -8.3641e-02, -8.2781e-01,\n",
      "          2.6076e-01, -1.1576e+00,  2.8229e-01, -1.9636e-01, -4.7078e-01,\n",
      "         -9.7715e-01,  0.0000e+00, -7.2585e-01, -1.2701e+00, -1.2469e+00,\n",
      "         -9.4952e-01],\n",
      "        [-5.1075e-01, -2.1196e-01, -2.9239e-01, -3.5990e-02,  2.8637e-02,\n",
      "          5.0985e-01,  3.8897e-01, -3.6825e-01, -2.5670e-01, -2.1912e-01,\n",
      "         -5.3302e-02,  9.8204e-01,  0.0000e+00, -2.1228e-01,  7.4502e-01,\n",
      "          3.2282e-01],\n",
      "        [ 5.2685e-02,  5.4402e-01,  2.3068e-01, -1.5487e-01,  5.4657e-02,\n",
      "         -5.1005e-01,  1.9036e-02, -6.8763e-01,  2.7716e-01,  4.0761e-01,\n",
      "          1.3207e-01,  3.3897e-01, -2.4673e-01,  0.0000e+00, -4.1792e-01,\n",
      "          2.4943e-01],\n",
      "        [ 4.6579e-01, -7.2150e-01, -1.3651e+00,  3.6657e-01,  1.8899e-02,\n",
      "         -5.2298e-01, -2.4691e-01,  6.3223e-01, -9.1569e-01, -1.6517e+00,\n",
      "          5.2585e-01, -1.0484e+00,  1.9722e-01, -1.4764e+00,  0.0000e+00,\n",
      "         -8.1765e-01],\n",
      "        [ 2.9428e-01,  1.6462e-01,  2.4482e-01, -8.4674e-01,  3.0731e-01,\n",
      "          4.0306e-01,  1.6490e-01,  3.2824e-01, -7.5495e-01,  1.9247e-01,\n",
      "         -1.0909e+00,  1.1931e+00,  5.0088e-01, -3.1072e-02,  2.6466e-01,\n",
      "          0.0000e+00]], device='cuda:0', requires_grad=True)\n",
      "Layer 4\n",
      "torch.Size([6, 16])\n",
      "Parameter containing:\n",
      "tensor([[-4.6672e-01,  1.0820e-01, -3.0757e-01, -1.4354e-01, -1.2798e-02,\n",
      "         -1.7653e-01, -1.1141e-01, -4.6122e-01, -4.7987e-01,  1.0495e-01,\n",
      "          6.8366e-03, -8.7301e-01, -3.1768e-01, -1.7302e-01, -4.7575e-01,\n",
      "         -2.4969e-01],\n",
      "        [-3.5450e-01, -9.8297e-02, -5.1959e-02, -1.5075e-02, -5.8476e-03,\n",
      "         -4.1118e-01, -3.5915e-02, -4.8202e-01, -1.4195e-01, -4.5463e-02,\n",
      "         -9.6716e-02, -1.1222e+00, -4.2809e-01, -8.2099e-02, -8.4520e-01,\n",
      "         -1.8646e-01],\n",
      "        [-2.4732e+00,  1.0774e-01, -1.4485e-01, -7.8011e-01, -1.3727e+00,\n",
      "         -1.3019e+00, -7.7306e-01, -1.3581e+00, -2.1952e+00, -6.2684e-02,\n",
      "         -1.1382e-01, -3.6716e+00, -2.0546e+00,  1.7342e-02, -9.5799e-01,\n",
      "          1.3930e-01],\n",
      "        [-1.8012e-01, -3.4979e-02, -6.6596e-02, -7.0550e-02, -1.7055e-01,\n",
      "         -3.0730e-01, -1.4190e-01, -3.3677e-01, -4.6406e-01,  7.2793e-04,\n",
      "         -2.2852e-01, -5.4277e-01, -1.7040e-01, -8.5604e-02, -4.7162e-01,\n",
      "         -1.0219e-01],\n",
      "        [-5.4357e-01,  8.8697e-03,  4.8701e-02, -1.4551e-01, -3.4975e-02,\n",
      "         -8.8522e-02, -1.3135e-01, -4.6641e-01, -1.7089e+00, -2.7469e-01,\n",
      "         -2.0043e-01, -5.2790e-01, -1.4968e-01, -2.7066e-01, -3.5198e-01,\n",
      "         -1.3649e-01],\n",
      "        [-3.9297e-01, -3.2234e-01, -1.4623e-01,  3.0772e-02,  6.4644e-02,\n",
      "         -2.3173e-02, -3.4647e-01, -3.8681e-01, -4.6614e-01, -1.3236e-01,\n",
      "         -1.0257e-01, -6.3515e-01, -2.3107e-01, -4.1816e-01, -4.1284e-01,\n",
      "         -3.3348e-01]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# check all weights\n",
    "\n",
    "param_list = [*model.parameters()]\n",
    "i = 0\n",
    "for lay in param_list:\n",
    "    i += 1\n",
    "    print(\"Layer {0}\".format(i))\n",
    "    print(lay.shape)\n",
    "    print(lay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0c209",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "314011da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to Recurrent ANN Models/RANN_13.pth\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"Recurrent ANN Models/\"\n",
    "model_name = \"RANN_13.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_dir + model_name)\n",
    "print(\"Saved PyTorch Model State to \" + model_dir + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d75a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207de82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
