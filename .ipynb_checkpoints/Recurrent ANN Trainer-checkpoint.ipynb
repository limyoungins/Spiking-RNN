{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43712388-c442-4021-bc0f-e898d4323078",
   "metadata": {},
   "source": [
    "# Analogous Recurrent ANN Trainer\n",
    "\n",
    "This notebook can be used to generate and train a recurrent ANN so that the weights can be copied over to a SNN with the same architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bf3b9-a1df-4af0-8122-1af8a12c46d8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf949556-79d1-4121-b739-50c0ca083181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af0fa6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502bd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data and prepare for vector conversion\n",
    "\n",
    "# load data\n",
    "f = open(\"Training Data\\\\train_5500.txt\")\n",
    "data = f.read()\n",
    "\n",
    "# split data into sentences\n",
    "sents = data.split('\\n')\n",
    "\n",
    "# split each sentence into words\n",
    "for i in range(len(sents)):\n",
    "    sents[i] = sents[i].split(' ')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bedb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare word2vector vocabulary (ty chatGPT :) )\n",
    "\n",
    "def read_word_vectors(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, vector_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * vector_size\n",
    "        word_vectors = {}\n",
    "\n",
    "        for _ in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == b' ':\n",
    "                    break\n",
    "                if ch != b'\\n':\n",
    "                    word.append(ch)\n",
    "            word = b''.join(word).decode('utf-8')\n",
    "            vector = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "\n",
    "    return word_vectors\n",
    "\n",
    "def get_word_vector(word, word_vectors):\n",
    "    return word_vectors.get(word)\n",
    "\n",
    "# Load the word vectors\n",
    "word_vectors = read_word_vectors('Word2Vec from Paper\\\\word2vec\\\\trunk\\\\vectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0f2683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "Word: example\n",
      "Vector: [-2.4861143   2.514228   -2.51763     1.1023496  -1.9080325   0.2741574\n",
      "  1.987924   -0.4649879  -1.3471494   3.144086   -1.9048123   1.5780666\n",
      " -0.08019609 -1.2507837  -2.59727    -0.28834745 -0.7053564  -2.2820096\n",
      " -1.7724434   1.340178    1.0592215   0.5715263  -0.39970756  0.19736235\n",
      "  0.37494832 -0.23648897 -0.5271788  -0.87137115 -0.16628984  0.47225156\n",
      " -2.3885674   0.3888019   1.7539101  -0.90970224  0.7972985  -0.8713628\n",
      " -0.74113584  3.1902182   0.655787   -0.20875123 -0.16770692  2.0293825\n",
      " -0.6267522   0.5787317   1.579219    1.4347987  -0.7990051  -0.19155246\n",
      " -1.1973183   1.641335    2.0438645  -0.9134578  -1.5359813   0.15457954\n",
      " -1.0635711   2.7433052   0.22127318 -1.5445443  -0.5777184  -1.1103141\n",
      "  0.9209189  -1.2365515   0.6689623   0.48781195]\n",
      "\n",
      "(64,)\n",
      "Word: word\n",
      "Vector: [-3.9669743   4.8188896  -2.954842    2.0429592   0.17152616  3.8808334\n",
      "  1.2621093   0.4342894   0.2122565  -1.6349137  -1.7049528  -1.1492262\n",
      "  4.1847644  -0.6373846   1.0669839   1.1486648  -1.1664046  -1.8581262\n",
      " -2.9110322   5.126338    1.3257608   1.2693102  -3.3614407  -1.2822112\n",
      "  5.1168675   0.03109401 -1.8007604  -3.9345162  -2.4310956  -0.53243566\n",
      " -1.5654114   5.0678897   0.30807668 -1.7529594   0.11373261 -1.500378\n",
      "  2.138824    2.2712488  -3.314855    1.1179694  -1.083036    4.7493787\n",
      " -0.26541588  2.1864147   1.2514437  -0.6617835  -0.79325044  0.36480644\n",
      "  1.0892671   2.2959142   1.2033259  -0.95589584 -3.275922    4.2712116\n",
      "  1.0834571   8.340132   -1.8334891  -4.2971954  -1.4262646  -0.5897796\n",
      "  1.8340505  -1.3767011  -2.5821886   0.5281493 ]\n",
      "\n",
      "(64,)\n",
      "Word: vector\n",
      "Vector: [-0.7018949  -1.2055086  -0.99426776  3.5984192  -4.02639     3.3043559\n",
      "  4.7709236   0.11311053  1.5363581   3.7943544  -2.3501387   0.8063334\n",
      "  1.763644    3.2983563  -1.8462338   0.11785336 -5.935734   -3.5553148\n",
      " -2.9720807   3.6282372   3.915432   -1.5889378   0.43966082  0.9580194\n",
      "  0.81886894  0.4643003  -1.9298434  -2.604461   -1.9676477   1.2810096\n",
      " -0.04105073 -0.43293315  0.5917811   0.7420474  -0.36222813 -0.32059428\n",
      "  0.5562149   4.2580957  -0.7984433  -1.7574553  -1.5736797  -0.9554701\n",
      " -1.9561586  -2.089061    5.257163    4.134999   -1.6924216   1.0676143\n",
      " -1.2609527   2.2166708  -4.2431235   1.9631809  -0.24528791 -3.893474\n",
      " -0.22060525 -0.9896199   2.372502   -0.5610109  -3.084976   -3.0171154\n",
      "  3.9851072   8.257224    2.1654563   0.9530224 ]\n",
      "\n",
      "(64,)\n",
      "Word: king\n",
      "Vector: [ 1.7957844  -4.6018395  -0.09757277 -0.4460061  -1.5987397  -2.9004903\n",
      "  3.2889428  -2.1755097   2.53303    -4.811589   -0.58885515 -4.725201\n",
      "  2.6883314   1.7700887   3.7403843   1.7053881   0.7724817   3.8277526\n",
      " -0.16695313 -3.2926173   0.37443164  1.6063944   4.571463   -0.5203922\n",
      "  4.212225    2.5528893   0.08803058  4.0971212  -3.355103    4.4759426\n",
      " -0.14250527  0.09280161 -1.7789161  -2.5967777  -1.6979797  -2.116493\n",
      "  0.4663919  -0.51319855 -1.4585135  -0.22882567  4.1775603   1.000381\n",
      "  1.5282894   0.9049099  -5.1817408   0.6439551   0.18710499  1.720523\n",
      "  1.1604469   0.82973677  3.413264   -0.24482794  2.3681939   2.7713485\n",
      " -2.234261    1.2545004  -3.0336092   2.9394715   1.0767974   3.0911672\n",
      "  4.843916    0.5172443   0.20076227  1.3695639 ]\n",
      "\n",
      "(64,)\n",
      "Word: queen\n",
      "Vector: [ 2.2802994  -4.099436    1.1793872  -2.431531   -2.373581   -0.20208801\n",
      "  6.1322994   2.2811089   0.37748232 -3.466364   -1.0488603  -2.5648284\n",
      "  3.181303    3.5120535  -0.25811362 -0.29654875 -1.8933567   1.5763551\n",
      "  2.9244006  -3.183846    1.6198385   1.0540639   7.024095   -2.5975845\n",
      "  2.43023    -2.4230723  -1.74276     1.0672995  -1.048975    5.5671277\n",
      " -1.8934703  -2.0161183  -3.3639965  -1.5083143  -2.5085785   1.8856598\n",
      " -0.7038164   0.02665352  1.8711231  -2.9604905   4.003653   -2.864039\n",
      " -1.2308829   0.7129145  -0.46878406 -2.268523    0.10041354 -0.87685627\n",
      "  1.0413698  -0.95126265  5.2159863   0.11423327 -1.6311998   0.38125977\n",
      " -2.8785517   2.7560174  -4.7247114  -0.418131    0.46117538  4.7174997\n",
      "  4.633895    1.015396    1.2275217   3.3712578 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test vocabulary\n",
    "\n",
    "# Define a list of words to convert to vectors\n",
    "words = ['example', 'word', 'vector', 'king', 'queen']\n",
    "\n",
    "# Convert words to vectors\n",
    "for word in words:\n",
    "    vector = get_word_vector(word, word_vectors)\n",
    "    print(vector.shape)\n",
    "    if vector is not None:\n",
    "        print(f\"Word: {word}\\nVector: {vector}\\n\")\n",
    "    else:\n",
    "        print(f\"Word: {word} not found in vocabulary.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "402e0cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liamr\\AppData\\Local\\Temp\\ipykernel_13184\\515567485.py:11: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  vecs.append(torch.from_numpy(vec))\n"
     ]
    }
   ],
   "source": [
    "# perform word to vector conversion\n",
    "\n",
    "# perform conversion\n",
    "vec_data = []\n",
    "for sent in sents[:-1]:\n",
    "    vecs = []\n",
    "    vecs.append(sent[0])\n",
    "    for word in sent[1:]:\n",
    "        try:\n",
    "            vec = get_word_vector(word, word_vectors)\n",
    "            vecs.append(torch.from_numpy(vec))\n",
    "        except:\n",
    "            pass\n",
    "    vecs.append(torch.zeros(64))\n",
    "    vec_data.append(vecs)\n",
    "\n",
    "# pad all sentences to length of longest sentence\n",
    "max_len = max([len(sent) for sent in vec_data])\n",
    "vec_data_pad = []\n",
    "for sent in vec_data:\n",
    "    pad_len = max_len - len(sent)\n",
    "    for i in range(pad_len):\n",
    "        sent.append(torch.zeros(64))\n",
    "    vec_data_pad.append(sent)\n",
    "vec_data = vec_data_pad\n",
    "\n",
    "# split into training and test data\n",
    "train_data = vec_data[:5000]\n",
    "test_data = vec_data[5000:-1]\n",
    "\n",
    "# NOTE: first word of each sentence is correct categ. -- last sentence is empty (excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b78ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataSet which can be used with PyTorch DataLoader\n",
    "\n",
    "ans_key = { 'DESC' :  0,\n",
    "            'ENTY' :  1,\n",
    "            'ABBR' :  2,\n",
    "            'HUM'  :  3,\n",
    "            'LOC'  :  4,\n",
    "            'NUM'  :  5 }\n",
    "\n",
    "class QuestionDataset(Dataset):\n",
    "    \"\"\" Question Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        data = list of (list of words -- first word is label)\n",
    "        \"\"\"\n",
    "        self.labels = []\n",
    "        self.sents = []\n",
    "        for sent in data:\n",
    "            lab_val = ans_key[sent[0].split(\":\")[0]]\n",
    "            lab_arr = torch.tensor(lab_val)\n",
    "            self.labels.append(lab_arr)\n",
    "            self.sents.append(sent[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sent = self.sents[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sent, label\n",
    "\n",
    "train_DSet = QuestionDataset(train_data)\n",
    "test_DSet = QuestionDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482a8daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.7112,  3.9187, -2.3859,  1.1708, -2.0570,  1.6660,  2.9410,  0.9965,\n",
      "          1.8875,  2.1013, -1.9081,  1.4046,  2.7684, -1.1823, -1.0634, -0.7583,\n",
      "          0.4377, -3.2159, -1.2020,  1.5030,  0.8661,  0.9120, -0.7257,  0.8150,\n",
      "         -1.4561,  1.0184, -3.1227, -0.7783, -0.3952, -0.3743, -1.6230, -0.2691,\n",
      "          1.9328, -0.7855,  1.0034, -0.2061,  0.9701,  2.3458, -0.4255,  1.8818,\n",
      "         -0.5267,  2.2756,  0.1094, -0.7850,  0.7655, -3.7835, -1.4916, -3.5457,\n",
      "         -1.0294, -0.0488,  0.8952,  0.5742,  0.1470, -0.7216, -0.5818,  2.2442,\n",
      "         -1.0479, -1.2184, -1.7076,  0.3324,  0.3778, -1.3552,  1.4898,  2.8329]]), tensor([[-0.1255, -0.4126, -1.5036,  0.5658, -1.5322, -0.3062,  2.7857,  0.9233,\n",
      "          0.3195, -0.4306, -2.1697,  0.2383, -0.0840, -0.7352,  1.3612, -0.5537,\n",
      "         -0.4857, -0.1546, -1.3385, -0.5487, -0.7021,  1.0826, -0.8593,  1.2547,\n",
      "         -1.8827,  0.5152, -2.4523, -0.1934, -1.8966, -1.3499,  1.0865, -0.1157,\n",
      "         -1.6995, -0.4025,  1.0100,  0.8994, -0.7290, -0.4267, -1.0596,  1.2209,\n",
      "         -0.7207,  0.3300,  0.3862, -0.9140,  0.4267, -0.3296,  1.6180, -2.5494,\n",
      "         -0.4436, -1.1370, -0.2836,  1.1734,  1.3698,  0.2817,  0.7277,  1.6965,\n",
      "         -1.6160, -0.3719, -1.2655,  1.1716, -1.4072, -0.3927,  0.8941,  0.8323]]), tensor([[ 5.0867e+00, -5.6811e+00,  1.0260e+00, -7.7513e-02,  8.3241e-01,\n",
      "          1.8181e+00, -7.5181e-01, -6.9512e-01, -1.0106e+00, -1.9495e+00,\n",
      "         -2.0785e+00, -8.5342e-01,  5.7848e-01, -5.7674e-01,  1.3226e+00,\n",
      "          7.6725e-01,  7.2153e-01,  4.5867e+00,  1.2998e+00, -3.0795e-01,\n",
      "          1.6018e+00, -2.1377e+00, -1.3543e+00,  1.1258e+00,  1.0474e+00,\n",
      "          1.4048e+00,  1.3730e+00, -9.0983e-01,  3.3841e+00, -1.3204e+00,\n",
      "          5.4364e-01,  6.4220e-01, -2.1381e+00, -1.3716e+00, -2.2054e+00,\n",
      "          1.9349e+00,  2.2844e+00, -2.1739e+00,  1.5685e+00,  1.8603e-01,\n",
      "          2.1605e+00, -4.0489e-01, -1.4935e+00,  1.6408e+00,  2.3671e-03,\n",
      "         -4.0034e-01,  2.3765e+00, -8.9457e-01,  3.5255e-01,  3.7269e-01,\n",
      "          6.0329e-01, -3.6403e-01, -2.1499e+00,  1.0936e+00, -6.0951e-03,\n",
      "         -1.6029e+00, -1.4292e+00,  1.5668e+00,  5.9511e-01,  7.8712e-01,\n",
      "          1.1983e+00,  2.8693e+00, -9.0018e-01, -9.7079e-01]]), tensor([[-1.4143,  0.0992, -2.3108, -0.5830, -1.0944, -0.3193,  1.9195,  0.2699,\n",
      "          1.2051, -1.0585, -2.2606,  0.1206, -1.9612, -1.0926, -0.2506, -0.0557,\n",
      "         -0.6838,  0.1701, -2.3205, -2.0101,  0.1891,  1.1598, -1.2154,  0.8956,\n",
      "         -2.3289,  0.7250, -2.0758, -0.4225, -1.2188, -1.7325,  0.6240,  0.6788,\n",
      "         -0.3515,  0.1365, -1.1027,  0.5103,  0.8050, -0.1385, -0.0412,  0.2296,\n",
      "         -0.2874, -1.0682, -0.4848, -1.2418,  0.4459,  0.6206,  1.7431, -1.8604,\n",
      "         -0.4546, -1.5489,  0.2660, -0.0554,  0.7522, -0.0210,  0.6118,  1.2925,\n",
      "         -0.5698,  0.2403, -1.0763,  0.9770, -0.7178, -0.5330,  1.1980,  1.9875]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n",
      "tensor([3])\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_DSet, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_DSet, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256c456",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a981bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (ff): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=48, bias=False)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (rnn): RNN(48, 16, bias=False)\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=6, bias=False)\n",
      "    (1): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" Builds recurrent neural network model \"\"\"\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(64, 48, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.rnn = nn.RNN(48, 16, nonlinearity='relu', bias=False)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(16, 6, bias=False),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, q):\n",
    "        \"\"\" Implements feed-forward then recurrent layer \"\"\"\n",
    "        ff_q = []\n",
    "        for word in q:\n",
    "            ff_q.append(self.ff(word))\n",
    "        ff_q = torch.stack(ff_q)\n",
    "        h_N = Variable(torch.zeros(1, 16)).to(device)\n",
    "        rnn_out = Variable(torch.zeros(1, 16)).to(device)\n",
    "        for word in ff_q:\n",
    "            if not torch.all(word.eq(0)):\n",
    "                rnn_out, h_N = self.rnn(word, h_N)\n",
    "            else:\n",
    "                break\n",
    "        output = self.out(rnn_out)\n",
    "        return output\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a554826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model\n",
    "\n",
    "param_list = [*model.parameters()]\n",
    "len(param_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f51ab5",
   "metadata": {},
   "source": [
    "## Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "873219a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss function and optimizer\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# define training routine\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    flag = False\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = torch.stack(X).to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error and optimize\n",
    "        optimizer.zero_grad()\n",
    "        \"\"\" now handled inside model -- keep to count empties\n",
    "        for word in X:\n",
    "            if torch.all(word.eq(0)):\n",
    "                print(\"FLAG!\")\n",
    "                print(X)\n",
    "                flag = True\n",
    "                break\n",
    "            else:\n",
    "                break\n",
    "        \"\"\"\n",
    "        if not flag:\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # clamp diagonal hidden-hidden weights of RNN layer to 0\n",
    "            model.rnn._parameters['weight_hh_l0'].data.diagonal().clamp_(min=0, max=0)\n",
    "\n",
    "            # print loss and accuracy at selected iterations\n",
    "            if batch % 1000 == 0:\n",
    "                loss, current = loss.item(), (batch + 1)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        else:\n",
    "            flag = False\n",
    "            pass\n",
    "\n",
    "# define test routine\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = torch.stack(X).to(device), y.to(device) #torch.FloatTensor(y).to(device)\n",
    "            pred = model(X)\n",
    "            target = y#.argmax(1)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f2121fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e3ddd2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.889511  [    1/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 1.670135  [ 1001/ 5000]\n",
      "loss: 0.656711  [ 2001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 1.153944  [ 3001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.564932  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 1.339316 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.870804  [    1/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.824387  [ 1001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 1.233719  [ 2001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 1.437806  [ 3001/ 5000]\n",
      "loss: 0.365734  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.174426 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.721823  [    1/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 1.375931  [ 1001/ 5000]\n",
      "loss: 0.506256  [ 2001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.473588  [ 3001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.354643  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.004462 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.831097  [    1/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.220364  [ 1001/ 5000]\n",
      "loss: 1.076833  [ 2001/ 5000]\n",
      "loss: 0.011565  [ 3001/ 5000]\n",
      "loss: 3.332847  [ 4001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 1.001756 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.673081  [    1/ 5000]\n",
      "loss: 0.548275  [ 1001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.711901  [ 2001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.817649  [ 3001/ 5000]\n",
      "loss: 1.203183  [ 4001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.953991 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.125529  [    1/ 5000]\n",
      "loss: 0.103115  [ 1001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.013243  [ 2001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.023590  [ 3001/ 5000]\n",
      "loss: 1.990889  [ 4001/ 5000]\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.933929 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.158073  [    1/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.001557  [ 1001/ 5000]\n",
      "loss: 0.651118  [ 2001/ 5000]\n",
      "loss: 0.065792  [ 3001/ 5000]\n",
      "loss: 1.040109  [ 4001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.916299 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.248466  [    1/ 5000]\n",
      "loss: 1.831616  [ 1001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.712148  [ 2001/ 5000]\n",
      "loss: 0.143268  [ 3001/ 5000]\n",
      "loss: 0.052290  [ 4001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.872306 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.071422  [    1/ 5000]\n",
      "loss: 0.095234  [ 1001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.370431  [ 2001/ 5000]\n",
      "loss: 1.172128  [ 3001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.677182  [ 4001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.944396 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.032223  [    1/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "loss: 0.009849  [ 1001/ 5000]\n",
      "loss: 2.267274  [ 2001/ 5000]\n",
      "loss: 0.697539  [ 3001/ 5000]\n",
      "loss: 0.206706  [ 4001/ 5000]\n",
      "FLAG!\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.895294 \n",
      "\n",
      "Done!\n",
      "\n",
      "Total epochs: 10\n"
     ]
    }
   ],
   "source": [
    "# perform training and test performance over epochs\n",
    "\n",
    "epochs = 40\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "total_epochs += epochs\n",
    "print(\"\\nTotal epochs: {0}\".format(total_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfc45f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case training takes a wrong turn\n",
    "temp_model_backup = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6b18c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "torch.Size([48, 64])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0558, -0.0961,  0.1141,  ...,  0.3348, -0.0348,  0.1416],\n",
      "        [-0.2190,  0.0531,  0.0683,  ..., -0.1284,  0.3164, -0.0308],\n",
      "        [ 0.2098,  0.0447, -0.2314,  ...,  0.2322,  0.0516, -0.1242],\n",
      "        ...,\n",
      "        [-0.1488,  0.0384,  0.1104,  ..., -0.1240, -0.1693, -0.1523],\n",
      "        [-0.0349, -0.2311,  0.0342,  ...,  0.3226, -0.1347, -0.0107],\n",
      "        [-0.0522, -0.1232, -0.5658,  ..., -0.1804,  0.0988,  0.0124]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Layer 2\n",
      "torch.Size([16, 48])\n",
      "Parameter containing:\n",
      "tensor([[ 2.1701e-01,  3.5191e-01, -2.4518e-01,  7.1747e-02,  3.5741e-01,\n",
      "         -2.2205e-03,  2.5283e-01,  6.6492e-02,  3.3923e-01,  2.8964e-01,\n",
      "         -4.9649e-02,  2.0624e-01, -9.9230e-04,  5.3794e-01, -1.5293e-01,\n",
      "          6.6349e-02, -3.5385e-02,  2.5008e-01,  1.1570e-01, -7.7591e-02,\n",
      "          1.9462e-01, -2.4641e-01,  1.2866e-01, -1.9357e-01,  4.3126e-01,\n",
      "          4.3302e-01, -8.3412e-02,  6.9357e-02,  3.8365e-01,  5.2135e-01,\n",
      "         -7.6288e-02,  1.6469e-01,  3.3174e-01,  1.1296e-01,  1.2947e-01,\n",
      "          1.4271e-01, -5.1719e-03, -4.1361e-01, -8.7930e-06, -4.1616e-01,\n",
      "         -2.5492e-02,  2.1601e-01,  1.8227e-01, -5.4668e-02,  4.7466e-02,\n",
      "         -2.4415e-01,  1.5468e-01,  2.4300e-02],\n",
      "        [-1.4987e-01,  9.7623e-02,  1.3024e-01,  2.4932e-01, -9.7535e-03,\n",
      "          4.0718e-01,  1.3138e-01,  1.7998e-01,  3.0988e-02,  2.4204e-01,\n",
      "          1.2341e-01,  9.6738e-02,  3.0214e-01, -1.5076e-01,  3.0266e-01,\n",
      "         -9.1423e-02,  3.6846e-01,  3.4987e-02, -1.0748e-01, -3.1898e-01,\n",
      "          2.2223e-02,  3.0245e-01, -2.9689e-01,  2.3987e-01,  4.3338e-01,\n",
      "         -8.3115e-02,  1.0245e-01, -1.1185e-01,  3.9596e-01, -4.7895e-02,\n",
      "          3.2529e-02,  3.9515e-01,  2.4405e-01,  9.4052e-02, -1.6341e-02,\n",
      "         -1.1693e-01,  8.5000e-02,  2.0517e-02, -1.0630e-01,  1.1328e-01,\n",
      "         -2.4755e-01,  4.5180e-01,  2.2098e-01,  4.6057e-02, -1.5139e-01,\n",
      "         -3.2861e-01, -7.9196e-02,  1.0772e-01],\n",
      "        [ 1.8929e-02,  2.0984e-01,  9.8672e-02,  8.9095e-02,  6.1061e-02,\n",
      "         -2.0392e-02, -2.8557e-01,  1.1628e-02,  1.6566e-01,  2.1221e-01,\n",
      "          5.5669e-02, -2.7104e-03, -1.4893e-02,  4.2395e-02,  1.5621e-01,\n",
      "          2.1252e-01, -8.0150e-02,  5.6681e-01, -2.3495e-01,  1.1362e-01,\n",
      "          1.1913e-02, -3.1208e-01, -2.0958e-01,  9.9645e-02,  2.2437e-01,\n",
      "          1.0264e-01, -1.5913e-01, -5.7465e-02,  2.0695e-01,  1.9396e-03,\n",
      "          5.1925e-03, -1.6462e-01,  2.2455e-01, -2.1108e-01,  2.0646e-01,\n",
      "          1.7703e-01, -2.0994e-02, -4.8501e-02, -7.9043e-02,  1.1792e-01,\n",
      "         -3.4041e-02, -1.6606e-01,  9.6101e-02, -3.0552e-01,  2.9029e-02,\n",
      "         -2.4744e-02, -4.6906e-02, -1.2963e-01],\n",
      "        [ 1.8685e-01, -1.7103e-01, -9.1116e-02,  1.4203e-01,  7.3481e-02,\n",
      "         -2.9334e-01,  2.3071e-01,  2.0846e-01, -1.8902e-01,  2.6111e-01,\n",
      "          1.6168e-01, -2.8495e-01, -4.1161e-01,  8.6181e-03,  7.8003e-02,\n",
      "          2.8659e-01, -1.6661e-01,  9.4188e-02, -2.4236e-02, -3.9075e-02,\n",
      "          3.9345e-01,  1.6929e-02, -2.4948e-01, -2.8436e-01,  2.6358e-03,\n",
      "         -2.4649e-01,  2.1178e-01,  3.6600e-01, -4.0100e-01, -2.4735e-02,\n",
      "          3.4377e-01, -1.4873e-01, -1.7745e-01,  4.9804e-02,  1.0180e-01,\n",
      "          2.0291e-01,  8.3749e-02,  3.0845e-01, -1.9959e-01, -1.3531e-03,\n",
      "         -5.5876e-03, -1.2628e-02,  1.8225e-01,  2.9311e-01,  2.2474e-01,\n",
      "          3.8968e-01,  2.6503e-01, -1.2645e-01],\n",
      "        [-3.6002e-01,  1.4891e-01,  2.9466e-01, -9.7312e-02,  2.8179e-01,\n",
      "          3.2914e-02,  2.1647e-01, -1.7108e-01,  2.4078e-01, -6.6715e-02,\n",
      "          3.0977e-01, -3.5799e-01, -1.8506e-01, -4.6001e-01,  2.2691e-01,\n",
      "          1.6067e-01,  2.5397e-01, -2.9176e-01,  5.5126e-01,  1.1230e-01,\n",
      "         -2.8030e-01, -1.2879e-01, -3.8760e-01,  1.6756e-01,  4.3666e-02,\n",
      "         -1.0179e-01,  2.3142e-01, -1.1274e-01, -1.7367e-01,  1.7716e-01,\n",
      "         -2.6325e-01,  1.1703e-01, -2.8538e-02, -1.3190e-02,  2.3563e-01,\n",
      "          2.2646e-01,  4.1936e-01,  2.0066e-01, -1.4845e-01, -9.2523e-02,\n",
      "         -1.0456e-01,  1.9190e-02,  1.9115e-01, -1.0526e-01,  2.8362e-01,\n",
      "         -1.5892e-01,  7.4754e-02, -3.9592e-01],\n",
      "        [ 1.6598e-02, -3.8536e-01,  3.0053e-01,  8.6097e-02,  7.8722e-02,\n",
      "         -2.4306e-01,  3.4369e-01,  1.0696e-01, -5.0955e-03,  4.4332e-02,\n",
      "         -2.9107e-01, -4.5090e-01, -1.5921e-01,  2.5278e-01, -9.7553e-02,\n",
      "          7.8437e-02, -3.6766e-02,  5.9588e-02, -1.8420e-01, -2.4855e-01,\n",
      "         -1.5811e-01,  1.5902e-01,  9.1951e-02,  2.6170e-01,  6.8803e-02,\n",
      "         -3.7351e-01,  1.8291e-01, -2.1526e-02,  1.2749e-01, -1.8567e-01,\n",
      "         -7.4962e-02,  1.7263e-01, -1.5306e-01,  2.9296e-01,  1.1282e-01,\n",
      "          7.9450e-02, -1.5962e-01,  2.6464e-01,  2.0794e-01,  1.5182e-01,\n",
      "          4.4846e-01, -2.7144e-02, -1.2369e-01,  1.1804e-01, -1.9059e-01,\n",
      "          1.3644e-02, -1.3045e-01, -8.2039e-02],\n",
      "        [ 1.2534e-01, -1.2720e-01, -1.0231e-01, -3.6485e-01, -1.0471e-01,\n",
      "         -3.2649e-01, -1.9273e-01,  9.1731e-03,  3.6480e-01, -2.9597e-02,\n",
      "         -5.0090e-01,  2.7058e-01, -2.4791e-02, -2.4351e-01, -9.2034e-02,\n",
      "          1.7232e-01,  1.8476e-01,  1.8242e-01, -1.1322e-01,  2.5182e-02,\n",
      "         -5.8451e-02, -3.3884e-01, -3.6388e-02,  4.4600e-01,  1.9710e-01,\n",
      "          8.7279e-02,  2.3435e-01,  1.1539e-01, -4.3127e-02,  2.6377e-01,\n",
      "         -3.3872e-01,  9.3541e-02,  2.0842e-01, -2.4133e-01,  2.2861e-01,\n",
      "          1.0959e-01, -4.0967e-01,  6.8914e-02, -2.4259e-02,  3.5602e-01,\n",
      "         -2.9733e-01,  2.2231e-01,  2.4342e-01, -3.1929e-01,  3.2323e-01,\n",
      "          2.7695e-01, -2.5195e-01,  4.3446e-02],\n",
      "        [ 3.3004e-02, -1.2945e-01, -2.7747e-02, -1.4868e-01, -1.3958e-01,\n",
      "          1.4934e-01,  7.6903e-02,  1.1032e-01, -3.4040e-01, -2.0391e-01,\n",
      "          1.6855e-01, -1.1263e-01, -1.7124e-01, -8.7453e-02, -4.3632e-01,\n",
      "          6.2521e-02, -3.7629e-01,  6.1110e-02,  4.8503e-02, -1.0865e-01,\n",
      "          2.8826e-01,  1.3742e-01, -1.6659e-01, -5.0786e-01,  3.7433e-01,\n",
      "          8.9872e-02,  1.6301e-01, -2.2900e-01,  5.7447e-02, -4.8818e-02,\n",
      "          2.4268e-02,  2.2704e-01, -2.7824e-02, -4.1165e-02, -1.2353e-01,\n",
      "          2.4131e-01, -1.3133e-01,  2.7391e-01,  1.8083e-02,  3.0169e-03,\n",
      "         -2.9415e-01,  1.6715e-02,  2.2401e-01, -7.3314e-02, -1.7884e-02,\n",
      "         -2.6393e-01,  1.2656e-01, -1.6568e-01],\n",
      "        [-2.9061e-01,  9.4788e-02,  3.2594e-01,  3.4521e-01, -2.4683e-02,\n",
      "         -3.2328e-01, -4.2978e-01,  1.3835e-01,  7.1403e-02, -1.3773e-01,\n",
      "          7.6213e-02,  2.8489e-01, -3.0577e-01,  3.9345e-01,  4.7429e-01,\n",
      "         -3.2877e-01,  6.8345e-02,  2.4661e-01, -6.8303e-02, -1.2089e-01,\n",
      "         -1.4894e-01, -5.4689e-01, -3.7291e-01, -3.2374e-01,  7.4450e-02,\n",
      "         -2.7988e-01,  4.7733e-01,  5.5518e-02, -1.1594e-01,  4.2059e-01,\n",
      "         -1.6775e-01, -2.4257e-01,  4.9443e-01, -4.3789e-01, -2.8016e-03,\n",
      "          1.4439e-01, -9.3202e-02, -3.6960e-01, -7.7017e-02, -1.3258e-01,\n",
      "         -1.7334e-01, -4.7644e-01,  1.7163e-01, -1.1932e-01,  2.9891e-02,\n",
      "          1.5941e-01, -8.2805e-02,  2.2752e-01],\n",
      "        [ 1.2138e-01,  2.5216e-01, -8.2475e-02, -7.0525e-02,  2.1668e-01,\n",
      "         -4.5361e-02,  1.5015e-02, -1.1964e-01,  5.8181e-02,  6.0336e-02,\n",
      "          1.4602e-01,  2.0846e-01, -2.4298e-01,  8.4953e-02, -3.2799e-02,\n",
      "         -2.0411e-01,  1.8332e-02,  1.0528e-01,  2.5396e-01, -3.3059e-01,\n",
      "          2.9307e-02,  2.8074e-01, -2.3400e-01, -1.9085e-01, -1.5420e-01,\n",
      "          1.8767e-01, -5.6804e-02,  7.5229e-02,  4.0396e-01,  1.0401e-01,\n",
      "          3.2348e-01, -1.9242e-01, -1.5294e-01, -6.0609e-02,  2.1068e-01,\n",
      "          1.0914e-01, -1.6463e-01,  1.4270e-02, -1.2368e-01, -1.6194e-01,\n",
      "          4.9309e-02,  1.2194e-01,  1.3464e-02,  2.9521e-01,  1.3266e-01,\n",
      "          1.3082e-01, -5.5685e-03, -7.4056e-02],\n",
      "        [-7.7625e-03, -1.4315e-01, -1.8668e-01,  2.1689e-01,  3.8242e-01,\n",
      "          3.0185e-01, -1.4364e-01, -1.0337e-01, -7.2814e-02,  2.7829e-01,\n",
      "          3.0447e-01,  2.0012e-01,  2.2584e-01,  2.3317e-01,  1.5866e-01,\n",
      "         -3.9874e-01,  3.4922e-01, -1.0887e-01,  2.9952e-03, -4.3647e-02,\n",
      "          6.1431e-02, -1.1822e-02,  1.8707e-01,  3.7473e-02, -2.6212e-01,\n",
      "         -5.2346e-02,  8.1660e-02,  2.8413e-01,  8.1522e-03,  6.7423e-02,\n",
      "         -3.6527e-01,  4.2776e-01, -6.4529e-02, -2.7702e-01, -1.2861e-01,\n",
      "          4.4309e-01,  2.6091e-01,  3.9032e-01, -2.1559e-01,  1.6298e-01,\n",
      "         -1.2631e-01, -1.1327e-01,  3.7278e-02, -1.3139e-01, -6.9441e-02,\n",
      "          2.5298e-01,  2.9237e-01,  3.7347e-01],\n",
      "        [ 3.5823e-01,  4.5364e-02,  1.2982e-01, -4.5196e-03,  7.5131e-02,\n",
      "         -3.0209e-01,  2.6477e-01,  2.3705e-01,  2.7730e-01, -1.1802e-01,\n",
      "         -1.9605e-01, -1.5859e-02,  1.3288e-01,  5.8370e-02, -5.1296e-01,\n",
      "         -8.0343e-02,  2.4891e-01,  2.5167e-01,  3.0981e-01, -1.4893e-01,\n",
      "          9.6527e-02, -6.6373e-02,  3.9810e-01, -1.2228e-01,  3.0498e-01,\n",
      "         -2.3373e-01,  4.9459e-02,  3.0404e-01,  1.4797e-01,  1.9825e-01,\n",
      "          8.7418e-02,  6.8506e-02,  3.2317e-01,  1.7917e-01,  1.9584e-02,\n",
      "          1.4086e-01,  3.3637e-02, -4.5460e-01, -3.4956e-01,  3.5902e-01,\n",
      "         -1.2053e-01,  1.9659e-01,  9.9802e-02, -6.4383e-02,  6.4275e-02,\n",
      "         -6.6410e-01, -3.6512e-02, -1.2891e-01],\n",
      "        [ 1.1368e-01, -1.8080e-01,  4.4120e-01, -2.8381e-01, -2.6294e-02,\n",
      "         -2.0554e-01, -4.9698e-01,  1.1150e-01, -7.3886e-02,  1.8685e-01,\n",
      "         -2.4513e-01, -2.0287e-01,  1.4285e-01, -1.2116e-01, -1.9110e-01,\n",
      "          3.3879e-01, -1.9792e-01,  1.2957e-01,  4.2568e-01,  3.3887e-01,\n",
      "          2.3889e-01, -2.5408e-01,  2.0156e-01,  1.1057e-01,  2.0024e-02,\n",
      "         -4.0371e-01, -1.2317e-01,  1.7263e-01,  1.1475e-01,  1.5588e-01,\n",
      "         -5.4068e-03, -2.7142e-01, -2.9388e-01,  2.0980e-01,  3.7963e-01,\n",
      "          1.2021e-01,  2.4160e-01, -1.1378e-01,  3.7659e-01,  2.6821e-01,\n",
      "         -1.2915e-01, -2.5845e-01,  1.9134e-01, -1.4090e-01, -3.8449e-01,\n",
      "         -6.2714e-02, -2.4403e-01,  1.6157e-01],\n",
      "        [-1.4058e-01, -9.6799e-02,  1.7427e-01,  4.8923e-01, -3.9875e-02,\n",
      "          2.6577e-01, -1.7476e-01, -4.0450e-01,  1.0894e-01, -4.1849e-01,\n",
      "          1.8005e-01,  1.1760e-01, -3.0452e-02, -3.8699e-01,  8.1091e-03,\n",
      "         -9.5834e-02,  1.7822e-01, -1.6208e-01,  2.1207e-01, -1.2390e-01,\n",
      "         -8.3057e-02,  8.8913e-02, -1.9367e-01,  1.7609e-01,  2.0496e-01,\n",
      "          1.8443e-01,  1.3123e-02,  2.4307e-01,  8.7012e-02, -8.3238e-02,\n",
      "          2.5931e-01,  3.0698e-01,  2.4924e-01,  1.1007e-01,  1.3453e-01,\n",
      "          1.0616e-01, -9.2403e-02,  1.1472e-01, -3.0667e-01, -3.1464e-02,\n",
      "         -1.4375e-01, -2.3326e-02, -2.5911e-01, -9.9995e-02, -2.8787e-01,\n",
      "         -2.8751e-01, -2.5440e-01, -1.0735e-01],\n",
      "        [ 5.9591e-02, -4.0150e-01,  2.6095e-01, -1.5483e-01,  2.3916e-01,\n",
      "         -4.3393e-02, -1.1875e-01, -1.2348e-01, -2.5429e-01,  1.8413e-02,\n",
      "         -1.6724e-01, -4.4946e-01,  1.1220e-01, -3.0023e-01, -3.9967e-02,\n",
      "         -8.3797e-02,  2.2375e-02,  4.2473e-02, -3.8658e-03, -2.7804e-01,\n",
      "          2.6653e-01,  2.0463e-01,  4.1386e-01,  1.8633e-01,  1.8023e-01,\n",
      "         -3.1281e-01,  1.8128e-01,  3.4716e-01,  5.5663e-02, -2.4093e-01,\n",
      "         -4.1859e-01, -2.9408e-01,  8.6023e-02,  8.9554e-02, -1.3686e-01,\n",
      "          1.7820e-01,  9.1087e-02,  1.8566e-01, -2.2607e-02,  4.6127e-02,\n",
      "         -4.0716e-01,  5.0181e-02, -9.4532e-02, -3.9447e-01, -2.7825e-01,\n",
      "          2.3765e-01,  3.9774e-01,  4.4154e-02],\n",
      "        [-1.8593e-01, -2.7430e-01, -9.4473e-02, -3.0082e-02, -5.2327e-02,\n",
      "          2.1317e-01, -1.9624e-01, -1.4233e-02, -1.7748e-01, -1.2020e-01,\n",
      "         -1.8125e-01,  4.6139e-03, -4.2636e-01,  2.5624e-01,  2.6503e-01,\n",
      "         -1.8822e-01, -2.4933e-01,  7.1821e-02, -2.5045e-01, -4.3600e-01,\n",
      "          1.3959e-01,  7.1547e-02, -2.2283e-01,  1.0908e-01, -4.1854e-01,\n",
      "         -4.1166e-02,  1.1324e-03, -6.5485e-02, -2.9259e-01, -3.4601e-03,\n",
      "          2.4665e-01,  5.1155e-02,  1.0922e-01,  4.9384e-03, -1.1326e-01,\n",
      "         -1.8003e-01, -7.9588e-02,  3.0855e-01, -3.3823e-01, -4.6327e-01,\n",
      "          1.5221e-02,  1.7080e-01, -1.7664e-01, -4.7964e-01, -7.5822e-02,\n",
      "          5.3185e-02,  4.8152e-02, -4.1469e-01]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Layer 3\n",
      "torch.Size([16, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000, -0.2986, -0.1280, -0.5652, -0.4368, -0.3511, -0.7071,  0.2452,\n",
      "         -0.0503,  0.0671, -0.8001,  0.0094, -0.3780, -0.6717, -0.8535, -0.1273],\n",
      "        [-0.1484,  0.0000, -0.0533, -1.0880,  0.4140, -0.0057, -0.2590, -0.0310,\n",
      "          0.1733, -0.5027,  0.1250,  0.1075, -0.2827,  0.4998, -0.1420, -0.1378],\n",
      "        [ 0.4488, -0.1986,  0.0000,  0.3157, -0.4391,  0.0920,  0.0504,  0.1160,\n",
      "         -0.0153,  0.4258, -0.2137,  0.4283, -0.0185,  0.2383, -0.3847,  0.1862],\n",
      "        [ 0.5448, -0.7499,  0.4571,  0.0000, -0.1903, -0.1193, -0.1142, -0.7588,\n",
      "          0.8846,  0.2647,  0.6556, -0.1743,  0.1014, -0.3609,  0.3047, -0.5941],\n",
      "        [-0.5151,  0.4424, -0.3175, -0.1702,  0.0000,  0.2234, -0.0535, -0.1381,\n",
      "         -0.0599, -0.6214,  0.2555, -0.2447,  0.1795,  0.0572,  0.2363,  0.3273],\n",
      "        [ 0.0898,  0.2202,  0.2752,  0.0215,  0.0374,  0.0000, -0.0172, -0.1761,\n",
      "          0.1237, -0.2110, -0.5727,  0.8721,  0.6312, -0.1739,  0.0522,  0.4217],\n",
      "        [-0.2162, -0.1777,  0.1687,  0.0841, -0.0350,  0.3501,  0.0000,  0.1193,\n",
      "          0.1888, -0.1106, -0.1798,  0.7948, -0.0241,  0.0966, -0.2007,  0.5541],\n",
      "        [ 0.4620, -0.3836,  0.0169, -0.3155, -0.8336, -0.0289,  0.3305,  0.0000,\n",
      "         -0.0516,  0.4576, -0.6237,  0.7153,  0.0201, -0.0187, -0.3455,  0.3342],\n",
      "        [-0.0643,  0.0293,  0.0458,  0.1804, -0.3074, -0.0376, -0.2852, -0.3779,\n",
      "          0.0000, -0.1186, -0.1953, -0.0221, -0.1477,  0.1841, -0.5317, -0.4011],\n",
      "        [ 1.0497,  0.0987,  0.4435,  0.3602, -0.6330,  0.1461,  0.0894,  0.4875,\n",
      "          0.3319,  0.0000, -0.0862,  0.8106, -0.3044,  0.2381, -0.7774,  0.0013],\n",
      "        [ 0.0536,  0.2408, -0.0936,  0.5825,  0.0350, -0.2448, -0.3884, -0.2696,\n",
      "          0.2318, -0.1214,  0.0000, -0.7379, -0.3800,  0.1753,  0.6979, -0.6151],\n",
      "        [ 0.8079, -0.1148, -0.0896, -0.5524, -0.7070, -0.0037, -0.5686,  0.0230,\n",
      "         -0.0036, -0.2132, -0.6752,  0.0000, -0.1609, -0.8645, -0.5769, -0.6157],\n",
      "        [-0.2342, -0.0518, -0.0754, -0.0886,  0.1477,  0.5971,  0.4353, -0.3050,\n",
      "         -0.0376, -0.2605, -0.2593,  0.7598,  0.0000, -0.2509,  0.5022,  0.1199],\n",
      "        [-0.0522,  0.6419,  0.2632, -0.1695,  0.2070, -0.3793,  0.0520,  0.0013,\n",
      "          0.1768,  0.4849, -0.0760,  0.2633, -0.3846,  0.0000, -0.2931,  0.1116],\n",
      "        [ 0.1305, -0.4577, -0.5517,  0.3934,  0.1617, -0.4174,  0.0989,  0.3123,\n",
      "         -0.5494, -0.9208,  0.3272, -0.7137,  0.0879, -0.9216,  0.0000, -0.5825],\n",
      "        [ 0.0508,  0.1298,  0.1820, -0.3635,  0.2870,  0.3813,  0.1381,  0.1500,\n",
      "         -0.2310,  0.1581, -0.6834,  1.0739,  0.3172,  0.1233, -0.0383,  0.0000]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Layer 4\n",
      "torch.Size([6, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.0553,  0.2517, -0.2993,  0.0449,  0.1360, -0.1427, -0.0351, -0.0917,\n",
      "         -0.1245,  0.2463,  0.1390, -0.3256, -0.1854, -0.0779, -0.1239, -0.0126],\n",
      "        [-0.1497,  0.0574,  0.0732,  0.1814,  0.1322, -0.2519,  0.1125, -0.2944,\n",
      "          0.1269,  0.0666,  0.0546, -0.4284, -0.2369,  0.0059, -0.1612,  0.0348],\n",
      "        [-0.6090,  0.2298, -0.2189, -0.4567, -0.6193, -0.5879, -0.2139, -0.7270,\n",
      "         -0.4444,  0.0860,  0.0038, -1.2138, -0.9143,  0.0973, -0.7007,  0.3841],\n",
      "        [ 0.0897,  0.1072,  0.1092,  0.0746, -0.1609, -0.1356,  0.0147, -0.0687,\n",
      "         -0.1664,  0.1042, -0.0891,  0.1022, -0.1278, -0.0034,  0.1072,  0.0405],\n",
      "        [-0.0574,  0.1121,  0.1266,  0.0147,  0.0872,  0.0603, -0.0369, -0.2802,\n",
      "         -0.3808, -0.1556, -0.0965, -0.0395, -0.0734, -0.0587,  0.0610,  0.0045],\n",
      "        [ 0.0961, -0.1313, -0.0282,  0.2128,  0.2446,  0.0728, -0.3499,  0.0469,\n",
      "         -0.1111, -0.0209,  0.0661, -0.3503, -0.1272, -0.3220,  0.0307, -0.2335]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# check all weights\n",
    "\n",
    "param_list = [*model.parameters()]\n",
    "i = 0\n",
    "for lay in param_list:\n",
    "    i += 1\n",
    "    print(\"Layer {0}\".format(i))\n",
    "    print(lay.shape)\n",
    "    print(lay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0c209",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "314011da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to Recurrent ANN Models/RANN_13.pth\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"Recurrent ANN Models/\"\n",
    "model_name = \"RANN_13.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_dir + model_name)\n",
    "print(\"Saved PyTorch Model State to \" + model_dir + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d75a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207de82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
